{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import sklearn.feature_extraction\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from skimage import io\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "%load_ext tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from fastai.layers import PixelShuffle_ICNR\n",
    "from skimage.transform import rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/x2_attention/128-256-CA-Better')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/workspace/data/Dhruv/pytorch/SuperResolution/Data'\n",
    "train_images = np.zeros((800*4, 256, 256, 3))\n",
    "i = 0\n",
    "for f in os.listdir(dir + '/' + 'Train'):\n",
    "    if(f.endswith(\".png\")):\n",
    "        train_images[i] = (skimage.transform.resize(\n",
    "            skimage.io.imread(dir + '/Train/' + f),(256, 256), mode ='constant'))\n",
    "        train_images[i+1] = rotate(train_images[i], angle=90)\n",
    "        train_images[i+2] = rotate(train_images[i], angle=180)\n",
    "        train_images[i+3] = rotate(train_images[i], angle=270)\n",
    "        i += 4     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/workspace/data/Dhruv/pytorch/SuperResolution/Flickr2K'\n",
    "train_images1 = np.zeros((2400, 512, 512, 3))\n",
    "test_images1 = np.zeros((250, 512, 512, 3))\n",
    "i = 0\n",
    "for f in os.listdir(dir):\n",
    "    if(f.endswith(\".png\")):\n",
    "        if(i<2400):\n",
    "            train_images1[i] = (skimage.transform.resize(\n",
    "                skimage.io.imread(dir + '/' + f),(512,512), mode ='constant'))\n",
    "            i += 1\n",
    "        elif(i>=2400):\n",
    "            test_images1[i-2400] = (skimage.transform.resize(\n",
    "            skimage.io.imread(dir + '/' + f),(512,512), mode ='constant'))\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract 64x64 patches from the images. 20 patches from each image.\n",
    "def patchExtract(images, patch_size=(128, 128), max_patches=2):\n",
    "    pe = sklearn.feature_extraction.image.PatchExtractor(patch_size=patch_size, max_patches = max_patches)\n",
    "    pe_fit = pe.fit(images)\n",
    "    pe_trans = pe.transform(images)\n",
    "    return pe_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/workspace/data/Dhruv/pytorch/SuperResolution/Data'\n",
    "test_images = np.zeros((100*4, 256, 256, 3))\n",
    "i = 0\n",
    "for f in os.listdir(dir + '/' + 'Validation'):\n",
    "    if(f.endswith(\".png\")):\n",
    "        test_images[i] = (skimage.transform.resize(\n",
    "            io.imread(dir + '/Validation/' + f),(256, 256), mode ='constant'))\n",
    "        test_images[i+1] = rotate(test_images[i], angle=90)\n",
    "        test_images[i+2] = rotate(test_images[i], angle=180)\n",
    "        test_images[i+3] = rotate(test_images[i], angle=270)\n",
    "        i += 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_images = np.concatenate((train_images, train_images1), axis=0)\n",
    "#test_images = np.concatenate((test_images, test_images1), axis=0)\n",
    "#train_images = patchExtract(train_images)\n",
    "#test_images = patchExtract(test_images)\n",
    "np.random.shuffle(train_images)\n",
    "np.random.shuffle(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bicubicDownsample(images, scale_factor=0.5):\n",
    "    out = torch.nn.functional.interpolate(images, scale_factor=scale_factor, mode='bicubic', align_corners=False)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = torch.from_numpy(train_images).permute(0,3,1,2)\n",
    "y_tr = y_tr.float()\n",
    "y_te = torch.from_numpy(test_images).permute(0,3,1,2)\n",
    "y_te = y_te.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_images\n",
    "del test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = bicubicDownsample(y_tr)\n",
    "x_tr = x_tr.float()\n",
    "x_te = bicubicDownsample(y_te)\n",
    "x_te = x_te.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = y_tr.contiguous()\n",
    "y_te = y_te.contiguous()\n",
    "print(x_tr.is_contiguous())\n",
    "print(x_te.is_contiguous())\n",
    "print(y_tr.is_contiguous())\n",
    "print(y_te.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.4438008788229047, 0.43282444892005567, 0.39990855960960575],\n",
    "                                     std=[0.26915865140736284, 0.25496531678120016, 0.2794830545396813])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom training dataset\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.x = x_tr\n",
    "        self.y = y_tr\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            return self.transform(self.x[index]), self.transform(self.y[index])\n",
    "        else:\n",
    "            return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "# Creating custom testing dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.x = x_te\n",
    "        self.y = y_te\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            return self.transform(self.x[index]), self.transform(self.y[index])\n",
    "        else:\n",
    "            return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(transform=normalize)\n",
    "test_dataset = TestDataset(transform=normalize)\n",
    "\n",
    "# Implementing train loader to split the data into batches\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, # data reshuffled at every epoch\n",
    "                          num_workers=2) # Use several subprocesses to load the data\n",
    "\n",
    "# Implementing train loader to split the data into batches\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, # data reshuffled at every epoch\n",
    "                          num_workers=2) # Use several subprocesses to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "n_samples = len(train_dataset)\n",
    "n_iterations = math.ceil(n_samples/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CALayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(CALayer, self).__init__()\n",
    "        # global average pooling: feature --> point\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        # feature channel downscale and upscale --> channel weight\n",
    "        self.conv_du = nn.Sequential(\n",
    "                nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.conv_du(y)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, nf, bias=True):\n",
    "        super().__init__()\n",
    "        # gc: growth channel, i.e. intermediate channels\n",
    "        self.conv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=bias)\n",
    "        self.bn1 = nn.InstanceNorm2d(nf)\n",
    "        self.conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=bias)\n",
    "        self.bn2 = nn.InstanceNorm2d(nf)\n",
    "        self.conv3 = nn.Conv2d(nf, nf, 3, 1, 1, bias=bias)\n",
    "        self.bn3 = nn.InstanceNorm2d(nf)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        self.att = CALayer(nf)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.lrelu(self.bn1(self.conv1(x)))\n",
    "        x2 = self.lrelu(self.bn2(self.conv2(x+x1)))\n",
    "        x3 = self.lrelu(self.bn3(self.conv3(x+x1+x2)))\n",
    "        x_att = self.att(x3)\n",
    "        x3 = x_att + x3\n",
    "        return x3 + x\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_blocks):\n",
    "        super().__init__()\n",
    "        self.upsample = PixelShuffle_ICNR(in_channels, in_channels//2, scale=2)\n",
    "        self.reduce_channels = nn.Conv2d(in_channels, out_channels, kernel_size = 1)\n",
    "        self.blocks = self.make_layer(in_channels, out_channels, n_blocks)\n",
    "    \n",
    "    def make_layer(self, in_channels, out_channels, n_blocks):\n",
    "        Blocks = []\n",
    "        Blocks.append(DecoderBlock(nf=in_channels))\n",
    "        Blocks.append(self.reduce_channels)\n",
    "        Blocks.append(DecoderBlock(nf=out_channels))\n",
    "        for _ in range(2, n_blocks):\n",
    "            Blocks.append(DecoderBlock(nf=out_channels))\n",
    "        return nn.Sequential(*Blocks)   \n",
    "            \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upsample(x1)\n",
    "        x = torch.cat((x2,x1), dim=1)\n",
    "        x = self.blocks(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DecoderLayerNoReduce(nn.Module):\n",
    "    def __init__(self, in_channels, n_blocks):\n",
    "        super().__init__()\n",
    "        self.upsample = PixelShuffle_ICNR(in_channels, in_channels//2, scale=2)\n",
    "        self.blocks = self.make_layer(in_channels, n_blocks)\n",
    "    \n",
    "    def make_layer(self, in_channels, n_blocks):\n",
    "        Blocks = []\n",
    "        for _ in range(n_blocks):\n",
    "            Blocks.append(DecoderBlock(nf=in_channels))\n",
    "        return nn.Sequential(*Blocks)\n",
    "        \n",
    "            \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upsample(x1)\n",
    "        x = torch.cat((x2,x1), dim=1)\n",
    "        x = self.blocks(x)\n",
    "        return x\n",
    "\n",
    "class FinalDecoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, n_blocks):\n",
    "        super().__init__()\n",
    "        self.upsample = PixelShuffle_ICNR(in_channels, in_channels, scale=2)\n",
    "        self.blocks = self.make_layer(in_channels)\n",
    "    \n",
    "    def make_layer(self, in_channels):\n",
    "        Blocks = []\n",
    "        Blocks.append(DecoderBlock(nf=in_channels))\n",
    "        Blocks.append(DecoderBlock(nf=in_channels))\n",
    "        return nn.Sequential(*Blocks)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = self.blocks(x)\n",
    "        return x\n",
    "    \n",
    "class ResUNet(nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = torchvision.models.resnet34(pretrained=True, progress=False)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "        \n",
    "        # Encoder path\n",
    "        self.in_layer = nn.Sequential(*self.base_layers[0:3])\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[4])\n",
    "        self.layer2 = nn.Sequential(*self.base_layers[5])\n",
    "        self.layer3 = nn.Sequential(*self.base_layers[6])\n",
    "        self.layer4 = nn.Sequential(*self.base_layers[7])\n",
    "        \n",
    "        # Cross path\n",
    "        self.cross = nn.Conv2d(3, 32 ,kernel_size=1)\n",
    "        self.cross_upsample = nn.Conv2d(3, 32 ,kernel_size=1)\n",
    "        \n",
    "        # Decoder path\n",
    "        self.up1 = DecoderLayer(512, 256, 3)\n",
    "        self.up2 = DecoderLayer(256, 128, 6)\n",
    "        self.up3 = DecoderLayer(128, 64, 4)\n",
    "        self.up4 = DecoderLayerNoReduce(64, 3)\n",
    "        self.up5 = DecoderLayerNoReduce(64, 3)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(64, 3, kernel_size=9, padding=4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Encoder path\n",
    "        x_inp = x\n",
    "        x_in = self.in_layer(x)\n",
    "        x_l1 = self.layer1(x_in)\n",
    "        x_l2 = self.layer2(x_l1)\n",
    "        x_l3 = self.layer3(x_l2)\n",
    "        x_l4 = self.layer4(x_l3)\n",
    "        \n",
    "        # Decoder path\n",
    "        x = self.up1(x_l4, x_l3)\n",
    "        x = self.up2(x, x_l2)\n",
    "        x = self.up3(x, x_l1)\n",
    "        x_upsample = F.interpolate(x_inp, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        x_inp = self.cross(x_inp)\n",
    "        x = self.up4(x, x_inp)\n",
    "        x_upsample = self.cross_upsample(x_upsample) \n",
    "        x = self.up5(x, x_upsample)\n",
    "        x = self.final_conv(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Loss Function (Perceptual Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = torchvision.models.vgg16(pretrained=True, progress=False)\n",
    "        features = model.features\n",
    "        self.relu1_2 = nn.Sequential()\n",
    "        self.relu2_2 = nn.Sequential()\n",
    "        self.relu3_3 = nn.Sequential()\n",
    "        self.relu4_3 = nn.Sequential()\n",
    "        for i in range(4):\n",
    "            self.relu1_2.add_module(name=\"relu1_2_\"+str(i+1), module=features[i])\n",
    "        for i in range(4, 9):\n",
    "            self.relu2_2.add_module(name=\"relu2_2_\"+str(i-3), module=features[i])\n",
    "        for i in range(9, 16):\n",
    "            self.relu3_3.add_module(name=\"relu3_3_\"+str(i-8), module=features[i])\n",
    "        for i in range(16, 23):\n",
    "            self.relu4_3.add_module(name=\"relu4_3_\"+str(i-15), module=features[i])      \n",
    "        # Setting requires_grad=False to fix the perceptual loss model parameters \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out_relu1_2 = self.relu1_2(x)\n",
    "        out_relu2_2 = self.relu2_2(out_relu1_2)\n",
    "        out_relu3_3 = self.relu3_3(out_relu2_2)\n",
    "        out_relu4_3 = self.relu4_3(out_relu3_3)\n",
    "        return out_relu1_2, out_relu2_2, out_relu3_3, out_relu4_3\n",
    "    \n",
    "# Function to calculate Gram matrix\n",
    "def gram(x):\n",
    "    (N, C, H, W) = x.shape\n",
    "    psy = x.view(N, C, H*W)\n",
    "    psy_T = psy.transpose(1, 2)\n",
    "    G = torch.bmm(psy, psy_T) / (C*H*W)  # Should we divide by N here? Or does batch matric multiplication do that on it's own? \n",
    "    return G\n",
    "\n",
    "def TVR(x, TV_WEIGHT=1e-6):\n",
    "    diff_i = torch.sum(torch.abs(x[:, :, :, 1:] - x[:, :, :, :-1]))\n",
    "    diff_j = torch.sum(torch.abs(x[:, :, 1:, :] - x[:, :, :-1, :]))\n",
    "    tv_loss = TV_WEIGHT*(diff_i + diff_j)\n",
    "    return tv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGGLoss = VGGPerceptualLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PerceptualLoss(x, y, STYLE_WEIGHT=1e-3, CONTENT_WEIGHT=1e-1, PIXEL_WEIGHT=1):\n",
    "    \n",
    "    x_features = VGGLoss(x)\n",
    "    y_features = VGGLoss(y)\n",
    "    \n",
    "    # Calculating per-pixel loss\n",
    "    C = y.shape[1]\n",
    "    H = y.shape[2]\n",
    "    W = y.shape[3]\n",
    "    pixel_loss =  F.l1_loss(x, y, reduction='sum') / (C*H*W)\n",
    "    #print(\"pixel loss= \",pixel_loss)\n",
    "    \n",
    "    # Calculating Total variation regularization value\n",
    "    tvr_loss = TVR(x)\n",
    "    #print(\"tvr loss= \", tvr_loss)\n",
    "    \n",
    "    # Calculating content loss\n",
    "    #weights = [0.25, 0.25, 0.25]\n",
    "    content_loss = 0.0\n",
    "    for i in range(2,4):\n",
    "        C = y_features[i].shape[1]\n",
    "        H = y_features[i].shape[2]\n",
    "        W = y_features[i].shape[3]\n",
    "        content_loss += (F.l1_loss(y_features[i], x_features[i], reduction='sum') / (C*H*W))\n",
    "        #print('c',content_loss)\n",
    "    #print(\"content loss= \", content_loss)\n",
    "    '''\n",
    "    # Calculating content loss\n",
    "    C = y_features[2].shape[1]\n",
    "    H = y_features[2].shape[2]\n",
    "    W = y_features[2].shape[3]\n",
    "    content_loss = F.l1_loss(x_features[2], y_features[2], reduction='sum') / (C*H*W)\n",
    "    #print(content_loss)\n",
    "    '''\n",
    "    # Calculating Style loss\n",
    "    style_loss = 0.0\n",
    "    for i in range(4):\n",
    "        C = y_features[i].shape[1]\n",
    "        H = y_features[i].shape[2]\n",
    "        W = y_features[i].shape[3]\n",
    "        style_loss += F.l1_loss(gram(x_features[i]), gram(y_features[i]), reduction='sum')\n",
    "        #print('s ',style_loss)\n",
    "    #print(\"style loss= \", style_loss)\n",
    "    total_loss = STYLE_WEIGHT*style_loss + CONTENT_WEIGHT*content_loss + PIXEL_WEIGHT*pixel_loss + tvr_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "example = iter(train_loader)\n",
    "example_data, example_target = example.next()\n",
    "example_data = F.interpolate(example_data, scale_factor=4, mode='bilinear', align_corners=False)\n",
    "loss = PerceptualLoss(example_data.to(device), example_target.to(device))\n",
    "del example\n",
    "del example_data\n",
    "del example_target\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing checkpoints\n",
    "def save_checkpoint_best(epoch, model):\n",
    "    print(\"Saving best model\")\n",
    "    PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/BestModel/best_model_\"+str(epoch)+\".pt\"\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, loss):  # Saving model in a way so we can load and start training again\n",
    "    PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/Models/model_\"+str(epoch)+\".pt\"\n",
    "    print(\"Saving model\")\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': loss,\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss_log = []\n",
    "val_loss_log = []\n",
    "model = ResUNet(3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, layer in model._modules.items():\n",
    "    if name in ['in_layer', 'layer1', 'layer2', 'layer3', 'layer4']:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=[]\n",
    "decoder=[]\n",
    "for name, layer in model._modules.items():\n",
    "    if name in ['in_layer', 'layer1', 'layer2', 'layer3', 'layer4']:\n",
    "        print(name)\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "            encoder.append(param)\n",
    "    elif name in ['cross', 'cross_upsample', 'up1', 'up2', 'up3', 'up4', 'up5', 'final_conv']:\n",
    "        print(name)\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "            decoder.append(param)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "                {'params': decoder},\n",
    "                {'params': encoder, 'lr': 0.00005}\n",
    "            ], lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = iter(train_loader)\n",
    "example_data, example_target = example.next()\n",
    "writer.add_graph(model, example_data.to(device))\n",
    "writer.close()\n",
    "del example\n",
    "del example_data\n",
    "del example_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_model():\n",
    "    least_val_loss = math.inf\n",
    "\n",
    "    for epoch in range(EPOCHS): \n",
    "        \n",
    "        beg_time = time.time() #To calculate time taken for each epoch\n",
    "\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            # Will run for 1000 iterations per epoch\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            out = model(x)\n",
    "            #Calculating loss\n",
    "            loss = PerceptualLoss(out, y)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update gradients\n",
    "            optimizer.step()\n",
    "            # Get training loss\n",
    "            train_loss += loss.item()\n",
    "        tr_loss_log.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(test_loader):\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                out = model(x)\n",
    "                #Calculating loss\n",
    "                loss = PerceptualLoss(out, y)\n",
    "                # Get validation loss\n",
    "                val_loss += loss.item()\n",
    "            val_loss_log.append(val_loss)\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        # Saving checkpoints\n",
    "        #save_checkpoint(epoch+1, model, optimizer, val_loss)\n",
    "        if(val_loss < least_val_loss):\n",
    "            save_checkpoint_best(epoch+1, model)\n",
    "            least_val_loss = val_loss\n",
    "\n",
    "        end_time = time.time()\n",
    "        print('Epoch: {:.0f}/{:.0f}, Time: {:.0f}m {:.0f}s, Train_Loss: {:.4f}, Val_loss: {:.4f}'.format(\n",
    "              epoch+1, EPOCHS, (end_time-beg_time)//60, (end_time-beg_time)%60, train_loss, val_loss))\n",
    "        writer.add_scalar('Training_loss', train_loss, epoch+1)\n",
    "        writer.add_scalar('Validation_loss', val_loss, epoch+1)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=runs/x2_attention    # To beat 267, 36   #Use lower lr? 10^-4   # Use new sagan with reduced columns (32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the final model\n",
    "PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/FinalModel/attn_x2/128-256-ca-norminp.pt\"\n",
    "print(\"Saving final model\")\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1,64,16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpool = nn.AvgPool2d(4, stride=4, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.shape)\n",
    "out = maxpool(a)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving just model\n",
    "PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/JustModels/final_trained_model_inorm128.pt\"\n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "model = ResUNet(3).to(device)\n",
    "model.load_state_dict(torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/FinalModel/attn_x2/128-256-old-sa.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the final model (fill in the final model epoch number)\n",
    "loaded_final_model = ResUNet(3).to(device)\n",
    "checkpoint = torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/FinalModel/modified-res-dense-inorm-big128.pt\")\n",
    "loaded_final_model.load_state_dict(checkpoint)\n",
    "#loaded_final_model.eval()\n",
    "model = loaded_final_model\n",
    "del loaded_final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the best model (fill in the best model epoch number)\n",
    "model = ResUNet(3).to(device)\n",
    "checkpoint = torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/BestModel/best_model_31.pt\") #3\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a model with desired epoch number\n",
    "loaded_model = ResUNet(3).to(device)\n",
    "checkpoint = torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/Models/model_8.pt\")\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model.eval()\n",
    "model = loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.array([0.26915865140736284, 0.25496531678120016, 0.2794830545396813]).reshape((3, 1, 1))\n",
    "mean = np.array([0.4438008788229047, 0.43282444892005567, 0.39990855960960575]).reshape((3, 1, 1))\n",
    "def unorm(data):\n",
    "    img = data.clone().numpy()\n",
    "    img = ((img * std + mean).transpose(1, 2, 0)*255.0).clip(0, 255).astype(\"uint8\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "ssim_noise = ssim(img, img_noise,\n",
    "                  data_range=img_noise.max() - img_noise.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "def PSNR(pred, gt):\n",
    "    height, width = pred.shape[:2]\n",
    "    imdff = pred - gt\n",
    "    mse = np.mean(imdff ** 2)\n",
    "    if mse == 0:\n",
    "        return 100\n",
    "    return 10 * math.log10((235*235)/mse)\n",
    "\n",
    "def _ycc(r, g, b): # in (0,255) range\n",
    "    y = 16 + (65.738*r)/256 + (129.057*g)/256 + (25.064*b)/256\n",
    "    return y\n",
    "\n",
    "def get_y(data):\n",
    "    data = 255 * data # Now scale by 255\n",
    "    data = data.astype(np.uint8)\n",
    "    #y = _ycc(data[:,:,0], data[:,:,1], data[:,:,2])\n",
    "    fin = skimage.color.convert_colorspace(data,'RGB','YCbCr')\n",
    "    y = fin[:,:,0]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/workspace/data/Dhruv/pytorch'\n",
    "for f in os.listdir(dir):\n",
    "    if(f==\"butterfly.png\"):\n",
    "        img = io.imread(dir +'/' + f)\n",
    "        break    \n",
    "img = np.asarray(img)\n",
    "img=img/255.0\n",
    "img = np.expand_dims(img, 0)\n",
    "img = torch.from_numpy(img).permute(0,3,1,2).float()\n",
    "out = bicubicDownsample(img)\n",
    "model.eval()\n",
    "out_2 = model(out.to(device))\n",
    "print(out_2.shape)\n",
    "print(img.shape)\n",
    "pred = out_2[0].cpu().detach().permute(1,2,0).numpy()\n",
    "gt = img[0].permute(1,2,0).numpy()\n",
    "print(PSNR(get_y(pred), get_y(gt)))\n",
    "print(ssim(get_y(pred), get_y(gt), data_range=(np.max(get_y(pred))-np.min(get_y(pred)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pred)\n",
    "plt.show()\n",
    "plt.imshow(gt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.image.imsave('bird_old_sa.png', np.clip(pred,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "example = iter(test_loader)\n",
    "example_data, example_target = example.next()\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "# Downsampled\n",
    "plt.subplot(2,2,1)\n",
    "plt.title('Downsampled')\n",
    "plt.imshow(example_data[0].permute(1,2,0))\n",
    "# Bi-linear upsampled\n",
    "out_1 = F.interpolate(example_data, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('Bi-linear upsampled')\n",
    "plt.imshow(out_1[0].permute(1,2,0))\n",
    "# Model prediction\n",
    "out_2 = model(example_data.to(device))\n",
    "plt.subplot(2,2,3)\n",
    "plt.title('Prediction')\n",
    "#plt.imshow(out_2[0].cpu().detach().permute(1,2,0))\n",
    "plt.imshow(unorm(out_2[0].cpu().detach()))\n",
    "# Ground truth\n",
    "plt.subplot(2,2,4)\n",
    "plt.title('Ground truth')\n",
    "plt.imshow(example_target[0].permute(1,2,0))\n",
    "plt.imshow(unorm(example_target[0]))\n",
    "pred = out_2[0].cpu().detach().permute(1,2,0).numpy()\n",
    "gt = example_target[0].permute(1,2,0).numpy()\n",
    "#print(PSNR(get_y(pred), get_y(gt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model1.eval()\n",
    "#example = iter(test_loader)\n",
    "#example_data, example_target = example.next()\n",
    "\n",
    "# old is only with 2 in perceptual, new is with 2 and 3 in perceptual\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(1,2,1)\n",
    "out_2 = model(example_data.to(device))\n",
    "plt.title('Prediction model')\n",
    "plt.imshow(out_2[15].cpu().detach().permute(1,2,0))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Prediction model1')\n",
    "out_2 = model1(example_data.to(device))\n",
    "plt.imshow(out_2[15].cpu().detach().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparision\n",
    "import matplotlib\n",
    "directory = 'compare/butterfly.png'\n",
    "img = skimage.io.imread(directory)\n",
    "img = img/255.0\n",
    "img = skimage.transform.resize(img,(64,64))\n",
    "img = np.asarray(img)\n",
    "img = np.expand_dims(img, 0)\n",
    "img = torch.from_numpy(img).permute(0,3,1,2).float()\n",
    "out = model(img.to(device))\n",
    "out_img = out[0].cpu().detach().permute(1,2,0).numpy()\n",
    "plt.imshow(out_img)\n",
    "matplotlib.image.imsave('compare/my_butterfly_4x.png', np.clip(out_img,0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature maps\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, _ = test_dataset[18]\n",
    "plt.imshow(data.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find activations for all the layers of the model\n",
    "for name, layer in model._modules.items():\n",
    "  layer.register_forward_hook(get_activation(name))\n",
    "data, _ = test_dataset[18]\n",
    "data.unsqueeze_(0)\n",
    "output = model(data.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = activation['up4'].squeeze().cpu()\n",
    "for idx in range(act.size(0)):\n",
    "    plt.imshow(act[idx])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
