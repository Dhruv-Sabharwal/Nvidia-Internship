{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import sklearn.feature_extraction\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "%load_ext tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/workspace/data/Dhruv/pytorch/SuperResolution/Data'\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = []\n",
    "for f in os.listdir(dir + '/' + 'Train'):\n",
    "    if(f.endswith(\".png\")):\n",
    "        train_images.append(skimage.transform.resize(\n",
    "            skimage.io.imread(dir + '/Train/' + f),(2048,1080), mode ='constant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = []\n",
    "for f in os.listdir(dir + '/' + 'Validation'):\n",
    "    if(f.endswith(\".png\")):\n",
    "        test_images.append(skimage.transform.resize(\n",
    "            skimage.io.imread(dir + '/Validation/' + f),(2048,1080), mode ='constant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract 64x64 patches from the images. 20 patches from each image.\n",
    "def patchExtract(images, patch_size=(64, 64), max_patches=20):\n",
    "    pe = sklearn.feature_extraction.image.PatchExtractor(patch_size=patch_size, max_patches = max_patches)\n",
    "    pe_fit = pe.fit(images)\n",
    "    pe_trans = pe.transform(images)\n",
    "    return pe_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images1 = np.asarray(train_images[:400], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_patches1 = patchExtract(train_images1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_images1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = np.asarray(test_images, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_patches = patchExtract(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images1 = np.asarray(train_images[400:], dtype=np.float32)\n",
    "train_images_patches2 = patchExtract(train_images1)\n",
    "train_images_patches = np.concatenate((train_images_patches1, train_images_patches2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images_patches.shape)\n",
    "print(test_images_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bicubicDownsample(images, scale_factor=0.5):\n",
    "    out = torch.nn.functional.interpolate(images, scale_factor=scale_factor, mode='bicubic', align_corners=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_images_patches1\n",
    "del train_images_patches2\n",
    "del train_images\n",
    "del test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = torch.from_numpy(train_images_patches).permute(0,3,1,2)\n",
    "y_tr = y_tr.float()\n",
    "y_te = torch.from_numpy(test_images_patches).permute(0,3,1,2)\n",
    "y_te = y_te.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_images_patches\n",
    "del test_images_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = bicubicDownsample(y_tr)\n",
    "x_tr = x_tr.float()\n",
    "x_te = bicubicDownsample(y_te)\n",
    "x_te = x_te.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = y_tr.contiguous()\n",
    "y_te = y_te.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_tr.is_contiguous())\n",
    "print(x_te.is_contiguous())\n",
    "print(y_tr.is_contiguous())\n",
    "print(y_te.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom training dataset\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = x_tr\n",
    "        self.y = y_tr\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "# Creating custom testing dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = x_te\n",
    "        self.y = y_te\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset()\n",
    "test_dataset = TestDataset()\n",
    "\n",
    "# Implementing train loader to split the data into batches\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, # data reshuffled at every epoch\n",
    "                          num_workers=2) # Use several subprocesses to load the data\n",
    "\n",
    "# Implementing train loader to split the data into batches\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, # data reshuffled at every epoch\n",
    "                          num_workers=2) # Use several subprocesses to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "n_samples = len(train_dataset)\n",
    "n_iterations = math.ceil(n_samples/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size = 1) # Skip connection\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip_x = self.skip(x)\n",
    "        conv_x = self.double_conv(x)\n",
    "        added_x = skip_x + conv_x  # Element-wise addition of skip connection filters and residual filters\n",
    "        return F.relu_(added_x) # Inplace functional version of relu\n",
    "    \n",
    "\n",
    "class PsUpsample(nn.Module): # Upsampling using pixel shuffle\n",
    "    \n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels//2, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = F.interpolate(x1, scale_factor=2, mode='nearest')\n",
    "        x1 = self.conv(x1)\n",
    "        x = torch.cat((x2, x1), dim=1)\n",
    "        return x\n",
    "    \n",
    "class UpConcatConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.reduce = nn.Conv2d(in_channels, out_channels, kernel_size=1)  # 1x1 convolution to reduce num of channels to half\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = F.interpolate(x1, scale_factor=2, mode='nearest')\n",
    "        x1 = self.reduce(x1)\n",
    "        # No need to crop the feature maps from the corresponding contracting layer since we using padding in DoubleConv\n",
    "        x = torch.cat((x2, x1), dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "    \n",
    "class InConv(nn.Module):  # First 9x9 convolution\n",
    "    \n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.inconv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=9, padding=4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.inconv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SR_UNet(nn.Module):  # Parameters = 20009219\n",
    "    \n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.inconv = InConv(in_channels)\n",
    "        self.dconv1 = DoubleConv(64, 128)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.dconv2 = DoubleConv(128, 256)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.dconv3 = DoubleConv(256, 512)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.dconv4 = DoubleConv(512, 1024)\n",
    "        self.up1 = UpConcatConv(1024, 512) # Reduction of C by 2^2 i.e. output channels = 256\n",
    "        self.up2 = UpConcatConv(512, 256) # Output channels = 64\n",
    "        self.up3 = UpConcatConv(256, 128)\n",
    "        self.outconvblock = nn.Sequential(                   # Input to this block has 128 channels and image size = input size\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),    # This block can be repeated for x4\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.outconv = nn.Conv2d(64, 3, kernel_size=9, padding=4)       \n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.inconv(x)\n",
    "        x1 = self.dconv1(x)\n",
    "        x2 = self.pool1(x1)\n",
    "        x2 = self.dconv2(x2)\n",
    "        x3 = self.pool2(x2)\n",
    "        x3 = self.dconv3(x3)\n",
    "        x4 = self.pool3(x3)\n",
    "        x4 = self.dconv4(x4)\n",
    "        x = self.up1(x4, x3)\n",
    "        x = self.up2(x, x2)\n",
    "        x = self.up3(x, x1)\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        x = self.outconvblock(x)\n",
    "        x = self.outconv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Loss Function (Perceptual Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = torchvision.models.vgg16(pretrained=True, progress=False)\n",
    "        features = model.features\n",
    "        self.relu2_2 = nn.Sequential()\n",
    "        for i in range(9):\n",
    "            self.relu2_2.add_module(name=\"relu2_2_\"+str(i+1), module=features[i])    \n",
    "        # Setting requires_grad=False to fix the perceptual loss model parameters \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out_relu2_2 = self.relu2_2(x)\n",
    "        return out_relu2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGGLoss = VGGPerceptualLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PerceptualLoss(x, y):\n",
    "    \n",
    "    x_features = VGGLoss(x)\n",
    "    y_features = VGGLoss(y)\n",
    "    \n",
    "    # Calculating feature loss\n",
    "    C = y_features.shape[1]\n",
    "    H = y_features.shape[2]\n",
    "    W = y_features.shape[3]\n",
    "    feature_loss = F.mse_loss(y_features, x_features, reduction='sum') / (C*H*W) # Here assuming square of Euclidean Norm = MSE Loss\n",
    "    return feature_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing checkpoints\n",
    "def save_checkpoint_best(epoch, model, optimizer, loss):\n",
    "    print(\"Saving best model\")\n",
    "    PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/BestModel/best_model_\"+str(epoch)+\".pt\"\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': loss,\n",
    "        \n",
    "            }, PATH)\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, loss):\n",
    "    PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/Models/model_\"+str(epoch)+\".pt\"\n",
    "    print(\"Saving model\")\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': loss,\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss_log = []\n",
    "val_loss_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SR_UNet(3).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Add weight decay?\n",
    "#loss = PerceptualLoss used directly in training loop\n",
    "\n",
    "example = iter(train_loader)\n",
    "example_data, example_target = example.next()\n",
    "writer.add_graph(model, example_data.to(device))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_model():\n",
    "\n",
    "  least_val_loss = math.inf\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "      \n",
    "      beg_time = time.time() #To calculate time taken for each epoch\n",
    "      \n",
    "      train_loss = 0.0\n",
    "      val_loss = 0.0\n",
    "      \n",
    "      for i, (x, y) in enumerate(train_loader):\n",
    "          x = x.to(device)\n",
    "          y = y.to(device)\n",
    "          # Will run for 1000 iterations per epoch\n",
    "          optimizer.zero_grad()\n",
    "          # Forward pass\n",
    "          out = model(x)\n",
    "          #Calculating loss\n",
    "          loss = PerceptualLoss(out, y)\n",
    "          # Backward pass\n",
    "          loss.backward()\n",
    "          # Update gradients\n",
    "          optimizer.step()\n",
    "          # Get training loss\n",
    "          train_loss += loss.item()\n",
    "      tr_loss_log.append(train_loss)\n",
    "      \n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "          for i, (x, y) in enumerate(test_loader):\n",
    "              x = x.to(device)\n",
    "              y = y.to(device)\n",
    "              out = model(x)\n",
    "              #Calculating loss\n",
    "              loss = PerceptualLoss(out, y)\n",
    "              # Get validation loss\n",
    "              val_loss += loss.item()\n",
    "          val_loss_log.append(val_loss)\n",
    "      model.train()\n",
    "      \n",
    "      # Saving checkpoints\n",
    "      save_checkpoint(epoch, model, optimizer, val_loss)\n",
    "      if(val_loss < least_val_loss):\n",
    "          save_checkpoint_best(epoch, model, optimizer, val_loss)\n",
    "          least_val_loss = val_loss\n",
    "          \n",
    "      end_time = time.time()\n",
    "      print('Epoch: {:.0f}/{:.0f}, Time: {:.0f}m {:.0f}s, Train_Loss: {:.4f}, Val_loss: {:.4f}'.format(\n",
    "          epoch+1, EPOCHS, (end_time-beg_time)//60, (end_time-beg_time)%60, train_loss, val_loss))\n",
    "      writer.add_scalar('Training_loss', train_loss, epoch*n_iterations+i)\n",
    "      writer.add_scalar('Validation_loss', val_loss, epoch*n_iterations+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \"/workspace/data/Dhruv/pytorch/SuperResolution/FinalModel/final_trained_model.pt\"\n",
    "print(\"Saving final model\")\n",
    "torch.save(model.state_dict(), FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = iter(test_loader)\n",
    "example_data, example_target = example.next()\n",
    "plt.imshow(example_data[15].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(example_data.to(device))\n",
    "plt.imshow(out[15].cpu().detach().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(example_target[15].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include data preprocessing, reduce model complexity, reduce learning rate or add decay rate, add dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
