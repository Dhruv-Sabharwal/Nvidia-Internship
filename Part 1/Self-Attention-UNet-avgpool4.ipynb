{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import sklearn.feature_extraction\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from skimage import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "%load_ext tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from fastai.layers import PixelShuffle_ICNR\n",
    "from skimage.transform import rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/x2_attention/64-128-SA-avgpool4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/workspace/data/Dhruv/pytorch/SuperResolution/Data'\n",
    "train_images = np.zeros((800*4, 128, 128, 3))\n",
    "i = 0\n",
    "for f in os.listdir(dir + '/' + 'Train'):\n",
    "    if(f.endswith(\".png\")):\n",
    "        train_images[i] = (skimage.transform.resize(\n",
    "            io.imread(dir + '/Train/' + f),(128, 128), mode ='constant'))\n",
    "        train_images[i+1] = rotate(train_images[i], angle=90)\n",
    "        train_images[i+2] = rotate(train_images[i], angle=180)\n",
    "        train_images[i+3] = rotate(train_images[i], angle=270)\n",
    "        i += 4     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/workspace/data/Dhruv/pytorch/SuperResolution/Flickr2K'\n",
    "train_images1 = np.zeros((2400, 512, 512, 3))\n",
    "test_images1 = np.zeros((250, 512, 512, 3))\n",
    "i = 0\n",
    "for f in os.listdir(dir):\n",
    "    if(f.endswith(\".png\")):\n",
    "        if(i<2400):\n",
    "            train_images1[i] = (skimage.transform.resize(\n",
    "                skimage.io.imread(dir + '/' + f),(512,512), mode ='constant'))\n",
    "            i += 1\n",
    "        elif(i>=2400):\n",
    "            test_images1[i-2400] = (skimage.transform.resize(\n",
    "            skimage.io.imread(dir + '/' + f),(512,512), mode ='constant'))\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract 64x64 patches from the images. 20 patches from each image.\n",
    "def patchExtract(images, patch_size=(128, 128), max_patches=2):\n",
    "    pe = sklearn.feature_extraction.image.PatchExtractor(patch_size=patch_size, max_patches = max_patches)\n",
    "    pe_fit = pe.fit(images)\n",
    "    pe_trans = pe.transform(images)\n",
    "    return pe_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/workspace/data/Dhruv/pytorch/SuperResolution/Data'\n",
    "test_images = np.zeros((100*4, 128, 128, 3))\n",
    "i = 0\n",
    "for f in os.listdir(dir + '/' + 'Validation'):\n",
    "    if(f.endswith(\".png\")):\n",
    "        test_images[i] = (skimage.transform.resize(\n",
    "            io.imread(dir + '/Validation/' + f),(128, 128), mode ='constant'))\n",
    "        test_images[i+1] = rotate(test_images[i], angle=90)\n",
    "        test_images[i+2] = rotate(test_images[i], angle=180)\n",
    "        test_images[i+3] = rotate(test_images[i], angle=270)\n",
    "        i += 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_images = np.concatenate((train_images, train_images1), axis=0)\n",
    "#test_images = np.concatenate((test_images, test_images1), axis=0)\n",
    "#train_images = patchExtract(train_images)\n",
    "#test_images = patchExtract(test_images)\n",
    "np.random.shuffle(train_images)\n",
    "np.random.shuffle(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bicubicDownsample(images, scale_factor=0.5):\n",
    "    out = torch.nn.functional.interpolate(images, scale_factor=scale_factor, mode='bicubic', align_corners=False)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = torch.from_numpy(train_images).permute(0,3,1,2)\n",
    "y_tr = y_tr.float()\n",
    "y_te = torch.from_numpy(test_images).permute(0,3,1,2)\n",
    "y_te = y_te.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_images\n",
    "del test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = bicubicDownsample(y_tr)\n",
    "x_tr = x_tr.float()\n",
    "x_te = bicubicDownsample(y_te)\n",
    "x_te = x_te.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = y_tr.contiguous()\n",
    "y_te = y_te.contiguous()\n",
    "print(x_tr.is_contiguous())\n",
    "print(x_te.is_contiguous())\n",
    "print(y_tr.is_contiguous())\n",
    "print(y_te.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom training dataset\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.x = x_tr\n",
    "        self.y = y_tr\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            return self.transform(self.x[index]), self.transform(self.y[index])\n",
    "        else:\n",
    "            return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "# Creating custom testing dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.x = x_te\n",
    "        self.y = y_te\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            return self.transform(self.x[index]), self.transform(self.y[index])\n",
    "        else:\n",
    "            return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset()\n",
    "test_dataset = TestDataset()\n",
    "\n",
    "# Implementing train loader to split the data into batches\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, # data reshuffled at every epoch\n",
    "                          num_workers=2) # Use several subprocesses to load the data\n",
    "\n",
    "# Implementing train loader to split the data into batches\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, # data reshuffled at every epoch\n",
    "                          num_workers=2) # Use several subprocesses to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "n_samples = len(train_dataset)\n",
    "n_iterations = math.ceil(n_samples/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "def snconv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "    return spectral_norm(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                                   stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias))\n",
    "\n",
    "class Self_Attn(nn.Module):\n",
    "    \"\"\" Self attention Layer\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(Self_Attn, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        if(in_channels>64):\n",
    "            self.out_dim = 16\n",
    "        else:\n",
    "            self.out_dim = 8\n",
    "        self.snconv1x1_theta = snconv2d(in_channels=in_channels, out_channels=self.out_dim, kernel_size=1, stride=1, padding=0)\n",
    "        self.snconv1x1_phi = snconv2d(in_channels=in_channels, out_channels=self.out_dim, kernel_size=1, stride=1, padding=0)\n",
    "        self.snconv1x1_g = snconv2d(in_channels=in_channels, out_channels=self.out_dim, kernel_size=1, stride=1, padding=0)\n",
    "        self.snconv1x1_attn = snconv2d(in_channels=self.out_dim, out_channels=in_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.maxpool = nn.AvgPool2d(4, stride=4, padding=0)\n",
    "        self.softmax  = nn.Softmax(dim=-1)\n",
    "        self.sigma = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps(B X C X W X H)\n",
    "            returns :\n",
    "                out : self attention value + input feature \n",
    "                attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        bs, ch, h, w = x.size()\n",
    "        # Theta path\n",
    "        theta = self.snconv1x1_theta(x)\n",
    "        theta = theta.view(bs, self.out_dim, h*w)\n",
    "        # Phi path\n",
    "        phi = self.snconv1x1_phi(x)\n",
    "        phi = self.maxpool(phi)\n",
    "        phi = phi.view(bs, self.out_dim, h*w//16)\n",
    "        # Attn map\n",
    "        attn = torch.bmm(theta.permute(0, 2, 1), phi)\n",
    "        attn = self.softmax(attn)\n",
    "        # g path\n",
    "        g = self.snconv1x1_g(x)\n",
    "        g = self.maxpool(g)\n",
    "        g = g.view(bs, self.out_dim, h*w//16)\n",
    "        # Attn_g\n",
    "        attn_g = torch.bmm(g, attn.permute(0, 2, 1))\n",
    "        attn_g = attn_g.view(bs, self.out_dim, h, w)\n",
    "        attn_g = self.snconv1x1_attn(attn_g)\n",
    "        # Out\n",
    "        out = x + self.sigma*attn_g\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class Self_Attn(nn.Module):\n",
    "    \"\"\" Self attention Layer\"\"\"\n",
    "    def __init__(self,in_dim):\n",
    "        super(Self_Attn,self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        #self.activation = activation\n",
    "        \n",
    "        if(in_dim>64):\n",
    "            out_dim = 16\n",
    "        else:\n",
    "            out_dim = 8\n",
    "        \n",
    "        self.query_conv = snconv2d(in_channels = in_dim , out_channels = out_dim , kernel_size= 1)\n",
    "        self.key_conv = snconv2d(in_channels = in_dim , out_channels = out_dim , kernel_size= 1)\n",
    "        self.value_conv = snconv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax  = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X W X H)\n",
    "            returns :\n",
    "                out : self attention value + input feature \n",
    "                attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        m_batchsize,C,width ,height = x.size()\n",
    "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n",
    "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n",
    "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
    "        attention = self.softmax(energy) # BX (N) X (N) \n",
    "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
    "\n",
    "        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
    "        out = out.view(m_batchsize,C,width,height)\n",
    "        \n",
    "        out = self.gamma*out + x\n",
    "        return out\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Channel Attention (CA) Layer\n",
    "class CALayer(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(CALayer, self).__init__()\n",
    "        # global average pooling: feature --> point\n",
    "        if(channel>64):\n",
    "            reduction=16\n",
    "        else:\n",
    "            reduction=8\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        # feature channel downscale and upscale --> channel weight\n",
    "        self.conv_du = nn.Sequential(\n",
    "                snconv2d(channel, channel // reduction, 1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                snconv2d(channel // reduction, channel, 1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.conv_du(y)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class DecoderBlockAttn(nn.Module):\n",
    "    def __init__(self, nf, bias=True):\n",
    "        super().__init__()\n",
    "        # gc: growth channel, i.e. intermediate channels\n",
    "        self.conv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=bias)\n",
    "        self.bn1 = nn.InstanceNorm2d(nf)\n",
    "        self.conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=bias)\n",
    "        self.bn2 = nn.InstanceNorm2d(nf)\n",
    "        self.conv3 = nn.Conv2d(nf, nf, 3, 1, 1, bias=bias)\n",
    "        self.bn3 = nn.InstanceNorm2d(nf)\n",
    "        self.lrelu = nn.ReLU(inplace=True)\n",
    "        self.att_sa = Self_Attn(nf)\n",
    "        self.att_ca = CALayer(nf)\n",
    "\n",
    "        torch.nn.init.kaiming_uniform_(self.conv1.weight, nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_uniform_(self.conv2.weight, nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_uniform_(self.conv3.weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.bn1(self.lrelu(self.conv1(x)))\n",
    "        x2 = self.bn2(self.lrelu(self.conv2(x+x1)))\n",
    "        x3 = self.bn3(self.lrelu(self.conv3(x+x1+x2)))\n",
    "        x_att1 = self.att_sa(x+x3)\n",
    "        x_att2 = self.att_ca(x+x3+x_att1)\n",
    "        return x_att2+x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, nf, bias=True):\n",
    "        super().__init__()\n",
    "        # gc: growth channel, i.e. intermediate channels\n",
    "        self.conv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=bias)\n",
    "        self.bn1 = nn.InstanceNorm2d(nf)\n",
    "        self.conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=bias)\n",
    "        self.bn2 = nn.InstanceNorm2d(nf)\n",
    "        self.conv3 = nn.Conv2d(nf, nf, 3, 1, 1, bias=bias)\n",
    "        self.bn3 = nn.InstanceNorm2d(nf)\n",
    "        self.lrelu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        torch.nn.init.kaiming_uniform_(self.conv1.weight, nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_uniform_(self.conv2.weight, nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_uniform_(self.conv3.weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.bn1(self.lrelu(self.conv1(x)))\n",
    "        x2 = self.bn2(self.lrelu(self.conv2(x+x1)))\n",
    "        x3 = self.bn3(self.lrelu(self.conv3(x+x1+x2)))\n",
    "        return x3 + x\n",
    "\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_blocks):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.reduce_channels = nn.Conv2d(in_channels, out_channels, kernel_size = 1)\n",
    "        self.blocks = self.make_layer(in_channels, out_channels, n_blocks)\n",
    "    \n",
    "    def make_layer(self, in_channels, out_channels, n_blocks):\n",
    "        Blocks = []\n",
    "        Blocks.append(DecoderBlock(nf=in_channels))\n",
    "        Blocks.append(self.reduce_channels)\n",
    "        Blocks.append(DecoderBlock(nf=out_channels))\n",
    "        Blocks.append(DecoderBlockAttn(nf=out_channels))\n",
    "        return nn.Sequential(*Blocks)\n",
    "        \n",
    "            \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upsample(x1)\n",
    "        x = torch.cat((x2,x1), dim=1)\n",
    "        x = self.blocks(x)\n",
    "        return x\n",
    "\n",
    "class DecoderLayerNoReduce(nn.Module):\n",
    "    def __init__(self, in_channels, n_blocks):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.blocks = self.make_layer(in_channels, n_blocks)\n",
    "    \n",
    "    def make_layer(self, in_channels, n_blocks):\n",
    "        Blocks = []\n",
    "        Blocks.append(DecoderBlock(nf=in_channels))\n",
    "        Blocks.append(DecoderBlock(nf=in_channels))\n",
    "        Blocks.append(DecoderBlockAttn(nf=in_channels))\n",
    "        return nn.Sequential(*Blocks)\n",
    "        \n",
    "            \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upsample(x1)\n",
    "        x = torch.cat((x2,x1), dim=1)\n",
    "        x = self.blocks(x)\n",
    "        return x\n",
    "\n",
    "'''\n",
    "class FinalDecoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, n_blocks):\n",
    "        super().__init__()\n",
    "        self.upsample = PixelShuffle_ICNR(in_channels, in_channels, scale=2)\n",
    "        self.blocks = self.make_layer(in_channels)\n",
    "    \n",
    "    def make_layer(self, in_channels):\n",
    "        Blocks = []\n",
    "        Blocks.append(DecoderBlock(nf=in_channels))\n",
    "        Blocks.append(DecoderBlock(nf=in_channels))\n",
    "        Blocks.append(DecoderBlockAttn(nf=in_channels))\n",
    "        return nn.Sequential(*Blocks)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = self.blocks(x)\n",
    "        return x\n",
    "'''   \n",
    "class ResUNet(nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = torchvision.models.resnet34(pretrained=True, progress=False)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "        \n",
    "        # Encoder path\n",
    "        self.in_layer = nn.Sequential(*self.base_layers[0:3])\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[4])\n",
    "        self.layer2 = nn.Sequential(*self.base_layers[5])\n",
    "        self.layer3 = nn.Sequential(*self.base_layers[6])\n",
    "        self.layer4 = nn.Sequential(*self.base_layers[7])\n",
    "        \n",
    "        # Cross path\n",
    "        self.cross = nn.Conv2d(3, 32 ,kernel_size=1)\n",
    "        self.cross_upsample = nn.Conv2d(3, 32 ,kernel_size=1)\n",
    "        \n",
    "        # Decoder path\n",
    "        self.up1 = DecoderLayer(512, 256, 3)\n",
    "        self.up2 = DecoderLayer(256, 128, 3) # should be 6\n",
    "        self.up3 = DecoderLayer(128, 64, 3) # should be 4\n",
    "        self.up4 = DecoderLayerNoReduce(64, 3)\n",
    "        self.up5 = DecoderLayerNoReduce(64, 3)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(64, 3, kernel_size=9, padding=4)\n",
    "      \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Encoder path\n",
    "        x_inp = x\n",
    "        x_in = self.in_layer(x)\n",
    "        x_l1 = self.layer1(x_in)\n",
    "        x_l2 = self.layer2(x_l1)\n",
    "        x_l3 = self.layer3(x_l2)\n",
    "        x_l4 = self.layer4(x_l3)\n",
    "        \n",
    "        # Decoder path\n",
    "        x = self.up1(x_l4, x_l3)\n",
    "        x = self.up2(x, x_l2)\n",
    "        x = self.up3(x, x_l1)\n",
    "        x_upsample = F.interpolate(x_inp, scale_factor=2, mode='bicubic', align_corners=False)\n",
    "        x_inp = self.cross(x_inp)\n",
    "        x = self.up4(x, x_inp)\n",
    "        x_upsample = self.cross_upsample(x_upsample) \n",
    "        x = self.up5(x, x_upsample)\n",
    "        x = self.final_conv(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Loss Function (Perceptual Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = torchvision.models.vgg16(pretrained=True, progress=False)\n",
    "        features = model.features\n",
    "        self.relu1_2 = nn.Sequential()\n",
    "        self.relu2_2 = nn.Sequential()\n",
    "        self.relu3_3 = nn.Sequential()\n",
    "        self.relu4_3 = nn.Sequential()\n",
    "        for i in range(4):\n",
    "            self.relu1_2.add_module(name=\"relu1_2_\"+str(i+1), module=features[i])\n",
    "        for i in range(4, 9):\n",
    "            self.relu2_2.add_module(name=\"relu2_2_\"+str(i-3), module=features[i])\n",
    "        for i in range(9, 16):\n",
    "            self.relu3_3.add_module(name=\"relu3_3_\"+str(i-8), module=features[i])\n",
    "        for i in range(16, 23):\n",
    "            self.relu4_3.add_module(name=\"relu4_3_\"+str(i-15), module=features[i])      \n",
    "        # Setting requires_grad=False to fix the perceptual loss model parameters \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out_relu1_2 = self.relu1_2(x)\n",
    "        out_relu2_2 = self.relu2_2(out_relu1_2)\n",
    "        out_relu3_3 = self.relu3_3(out_relu2_2)\n",
    "        out_relu4_3 = self.relu4_3(out_relu3_3)\n",
    "        return out_relu1_2, out_relu2_2, out_relu3_3, out_relu4_3\n",
    "    \n",
    "# Function to calculate Gram matrix\n",
    "def gram(x):\n",
    "    (N, C, H, W) = x.shape\n",
    "    psy = x.view(N, C, H*W)\n",
    "    psy_T = psy.transpose(1, 2)\n",
    "    G = torch.bmm(psy, psy_T) / (C*H*W)  # Should we divide by N here? Or does batch matric multiplication do that on it's own? \n",
    "    return G\n",
    "\n",
    "def TVR(x, TV_WEIGHT=1e-6):\n",
    "    diff_i = torch.sum(torch.abs(x[:, :, :, 1:] - x[:, :, :, :-1]))\n",
    "    diff_j = torch.sum(torch.abs(x[:, :, 1:, :] - x[:, :, :-1, :]))\n",
    "    tv_loss = TV_WEIGHT*(diff_i + diff_j)\n",
    "    return tv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGGLoss = VGGPerceptualLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PerceptualLoss(x, y, STYLE_WEIGHT=1e-3, CONTENT_WEIGHT=1e-1, PIXEL_WEIGHT=1):\n",
    "    \n",
    "    x_features = VGGLoss(x)\n",
    "    y_features = VGGLoss(y)\n",
    "    \n",
    "    # Calculating per-pixel loss\n",
    "    C = y.shape[1]\n",
    "    H = y.shape[2]\n",
    "    W = y.shape[3]\n",
    "    pixel_loss =  F.l1_loss(x, y, reduction='sum') / (C*H*W)\n",
    "    #print(\"pixel loss= \",pixel_loss)\n",
    "    \n",
    "    # Calculating Total variation regularization value\n",
    "    tvr_loss = TVR(x)\n",
    "    #print(\"tvr loss= \", tvr_loss)\n",
    "    \n",
    "    # Calculating content loss\n",
    "    #weights = [0.25, 0.25, 0.25]\n",
    "    content_loss = 0.0\n",
    "    for i in range(2,4):\n",
    "        C = y_features[i].shape[1]\n",
    "        H = y_features[i].shape[2]\n",
    "        W = y_features[i].shape[3]\n",
    "        content_loss += (F.l1_loss(y_features[i], x_features[i], reduction='sum') / (C*H*W))\n",
    "        #print('c',content_loss)\n",
    "    #print(\"content loss= \", content_loss)\n",
    "    '''\n",
    "    # Calculating content loss\n",
    "    C = y_features[2].shape[1]\n",
    "    H = y_features[2].shape[2]\n",
    "    W = y_features[2].shape[3]\n",
    "    content_loss = F.l1_loss(x_features[2], y_features[2], reduction='sum') / (C*H*W)\n",
    "    #print(content_loss)\n",
    "    '''\n",
    "    # Calculating Style loss\n",
    "    style_loss = 0.0\n",
    "    for i in range(4):\n",
    "        C = y_features[i].shape[1]\n",
    "        H = y_features[i].shape[2]\n",
    "        W = y_features[i].shape[3]\n",
    "        style_loss += F.l1_loss(gram(x_features[i]), gram(y_features[i]), reduction='sum')\n",
    "        #print('s ',style_loss)\n",
    "    #print(\"style loss= \", style_loss)\n",
    "    total_loss = STYLE_WEIGHT*style_loss + CONTENT_WEIGHT*content_loss + PIXEL_WEIGHT*pixel_loss + tvr_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "example = iter(train_loader)\n",
    "example_data, example_target = example.next()\n",
    "example_data = F.interpolate(example_data, scale_factor=4, mode='bilinear', align_corners=False)\n",
    "loss = PerceptualLoss(example_data.to(device), example_target.to(device))\n",
    "del example\n",
    "del example_data\n",
    "del example_target\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing checkpoints\n",
    "def save_checkpoint_best(epoch, model):\n",
    "    print(\"Saving best model\")\n",
    "    PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/BestModel/best_model_\"+str(epoch)+\".pt\"\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, loss):  # Saving model in a way so we can load and start training again\n",
    "    PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/Models/model_\"+str(epoch)+\".pt\"\n",
    "    print(\"Saving model\")\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': loss,\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model\n",
    "tr_loss_log = []\n",
    "val_loss_log = []\n",
    "model = ResUNet(3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, layer in model._modules.items():\n",
    "    if name in ['in_layer', 'layer1', 'layer2', 'layer3', 'layer4']:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=[]\n",
    "decoder=[]\n",
    "for name, layer in model._modules.items():\n",
    "    if name in ['in_layer', 'layer1', 'layer2', 'layer3', 'layer4']:\n",
    "        print(name)\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "            encoder.append(param)\n",
    "    elif name in ['cross', 'cross_upsample', 'up1', 'up2', 'up3', 'up4', 'up5', 'final_conv']:\n",
    "        print(name)\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "            decoder.append(param)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "                {'params': decoder},\n",
    "                {'params': encoder, 'lr': 0.00005}\n",
    "            ], lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = iter(train_loader)\n",
    "example_data, example_target = example.next()\n",
    "writer.add_graph(model, example_data.to(device))\n",
    "writer.close()\n",
    "del example\n",
    "del example_data\n",
    "del example_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_model():\n",
    "    least_val_loss = math.inf\n",
    "\n",
    "    for epoch in range(EPOCHS): \n",
    "        \n",
    "        beg_time = time.time() #To calculate time taken for each epoch\n",
    "\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            # Will run for 1000 iterations per epoch\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            out = model(x)\n",
    "            #Calculating loss\n",
    "            loss = PerceptualLoss(out, y)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update gradients\n",
    "            optimizer.step()\n",
    "            # Get training loss\n",
    "            train_loss += loss.item()\n",
    "        tr_loss_log.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(test_loader):\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                out = model(x)\n",
    "                #Calculating loss\n",
    "                loss = PerceptualLoss(out, y)\n",
    "                # Get validation loss\n",
    "                val_loss += loss.item()\n",
    "            val_loss_log.append(val_loss)\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        # Saving checkpoints\n",
    "        #save_checkpoint(epoch+1, model, optimizer, val_loss)\n",
    "        if(val_loss < least_val_loss):\n",
    "            save_checkpoint_best(epoch+1, model)\n",
    "            least_val_loss = val_loss\n",
    "\n",
    "        end_time = time.time()\n",
    "        print('Epoch: {:.0f}/{:.0f}, Time: {:.0f}m {:.0f}s, Train_Loss: {:.4f}, Val_loss: {:.4f}'.format(\n",
    "              epoch+1, EPOCHS, (end_time-beg_time)//60, (end_time-beg_time)%60, train_loss, val_loss))\n",
    "        writer.add_scalar('Training_loss', train_loss, epoch+1)\n",
    "        writer.add_scalar('Validation_loss', val_loss, epoch+1)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=runs/x2_attention    # To beat 267, 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the final model\n",
    "PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/FinalModel/attn_x2/128-256-avgpool4.pt\"\n",
    "print(\"Saving final model\")\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving just model\n",
    "PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/JustModels/final_trained_model_inorm128.pt\"\n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "model = ResUNet(3).to(device)\n",
    "model.load_state_dict(torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/FinalModel/36433/model-128-256.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the final model (fill in the final model epoch number)\n",
    "loaded_final_model = ResUNet(3).to(device)\n",
    "checkpoint = torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/FinalModel/modified-res-dense-inorm-big128.pt\")\n",
    "loaded_final_model.load_state_dict(checkpoint)\n",
    "#loaded_final_model.eval()\n",
    "model = loaded_final_model\n",
    "del loaded_final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the best model (fill in the best model epoch number)\n",
    "del model\n",
    "model = ResUNet(3).to(device)\n",
    "model.load_state_dict(torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/BestModel/best_model_74.pt\")) #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a model with desired epoch number\n",
    "loaded_model = ResUNet(3).to(device)\n",
    "checkpoint = torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/Models/model_8.pt\")\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model.eval()\n",
    "model = loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n",
    "mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
    "def unorm(data):\n",
    "    img = data.clone().numpy()\n",
    "    img = ((img * std + mean).transpose(1, 2, 0)*255.0).clip(0, 255).astype(\"uint8\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "example = iter(test_loader)\n",
    "example_data, example_target = example.next()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "# Downsampled\n",
    "plt.subplot(2,2,1)\n",
    "plt.title('Downsampled')\n",
    "plt.imshow(example_data[0].permute(1,2,0))\n",
    "# Bi-linear upsampled\n",
    "out_1 = F.interpolate(example_data, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('Bi-linear upsampled')\n",
    "plt.imshow(out_1[0].permute(1,2,0))\n",
    "# Model prediction\n",
    "out_2 = model(example_data.to(device))\n",
    "plt.subplot(2,2,3)\n",
    "plt.title('Prediction')\n",
    "plt.imshow(out_2[0].cpu().detach().permute(1,2,0))\n",
    "# Ground truth\n",
    "plt.subplot(2,2,4)\n",
    "plt.title('Ground truth')\n",
    "plt.imshow(example_target[0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model1.eval()\n",
    "#example = iter(test_loader)\n",
    "#example_data, example_target = example.next()\n",
    "\n",
    "# old is only with 2 in perceptual, new is with 2 and 3 in perceptual\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(1,2,1)\n",
    "out_2 = model(example_data.to(device))\n",
    "plt.title('Prediction model')\n",
    "plt.imshow(out_2[15].cpu().detach().permute(1,2,0))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Prediction model1')\n",
    "out_2 = model1(example_data.to(device))\n",
    "plt.imshow(out_2[15].cpu().detach().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparision\n",
    "import matplotlib\n",
    "directory = 'compare/butterfly.png'\n",
    "img = skimage.io.imread(directory)\n",
    "img = img/255.0\n",
    "#img = skimage.transform.resize(img,(64,64))\n",
    "img = np.asarray(img)\n",
    "img = np.expand_dims(img, 0)\n",
    "img = torch.from_numpy(img).permute(0,3,1,2).float()\n",
    "out = model(img.to(device))\n",
    "out_img = out[0].cpu().detach().permute(1,2,0).numpy()\n",
    "plt.imshow(out_img)\n",
    "#matplotlib.image.imsave('compare/my_butterfly_4x.png', np.clip(out_img,0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature maps\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, _ = test_dataset[32]\n",
    "plt.imshow(data.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find activations for all the layers of the model\n",
    "for name, layer in model._modules.items():\n",
    "  layer.register_forward_hook(get_activation(name))\n",
    "data, _ = test_dataset[32]\n",
    "data.unsqueeze_(0)\n",
    "output = model(data.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = activation['up5'].squeeze().cpu()\n",
    "for idx in range(act.size(0)):\n",
    "    plt.imshow(act[idx])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = torchvision.models.resnet34(pretrained=True, progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.layer3[0].conv1.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.up3.blocks[0].conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
