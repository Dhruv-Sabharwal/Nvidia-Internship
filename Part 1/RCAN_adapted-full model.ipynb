{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import sklearn.feature_extraction\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from skimage import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "%load_ext tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from fastai.layers import PixelShuffle_ICNR\n",
    "from skimage.transform import rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/rcan/64-128-fullrcan-64patches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/workspace/data/Dhruv/pytorch/SuperResolution/Data'\n",
    "train_images = np.zeros((800, 700, 700, 3))\n",
    "i = 0\n",
    "for f in os.listdir(dir + '/' + 'Train'):\n",
    "    if(f.endswith(\".png\")):\n",
    "        train_images[i] = (skimage.transform.resize(\n",
    "            io.imread(dir + '/Train/' + f),(700, 700), mode ='constant'))\n",
    "        i += 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/workspace/data/Dhruv/pytorch/SuperResolution/Flickr2K'\n",
    "train_images1 = np.zeros((2400, 512, 512, 3))\n",
    "test_images1 = np.zeros((250, 512, 512, 3))\n",
    "i = 0\n",
    "for f in os.listdir(dir):\n",
    "    if(f.endswith(\".png\")):\n",
    "        if(i<2400):\n",
    "            train_images1[i] = (skimage.transform.resize(\n",
    "                skimage.io.imread(dir + '/' + f),(512,512), mode ='constant'))\n",
    "            i += 1\n",
    "        elif(i>=2400):\n",
    "            test_images1[i-2400] = (skimage.transform.resize(\n",
    "            skimage.io.imread(dir + '/' + f),(512,512), mode ='constant'))\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract 64x64 patches from the images. 20 patches from each image.\n",
    "def patchExtract(images, patch_size=(128, 128), max_patches=8):\n",
    "    pe = sklearn.feature_extraction.image.PatchExtractor(patch_size=patch_size, max_patches = max_patches)\n",
    "    pe_fit = pe.fit(images)\n",
    "    pe_trans = pe.transform(images)\n",
    "    return pe_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/workspace/data/Dhruv/pytorch/SuperResolution/Data'\n",
    "test_images = np.zeros((100, 700, 700, 3))\n",
    "i = 0\n",
    "for f in os.listdir(dir + '/' + 'Validation'):\n",
    "    if(f.endswith(\".png\")):\n",
    "        test_images[i] = (skimage.transform.resize(\n",
    "            io.imread(dir + '/Validation/' + f),(700, 700), mode ='constant'))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_images = np.concatenate((train_images, train_images1), axis=0)\n",
    "#test_images = np.concatenate((test_images, test_images1), axis=0)\n",
    "train_images = patchExtract(train_images)\n",
    "test_images = patchExtract(test_images)\n",
    "#np.random.shuffle(train_images)\n",
    "#np.random.shuffle(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bicubicDownsample(images, scale_factor=0.5):\n",
    "    out = torch.nn.functional.interpolate(images, scale_factor=scale_factor, mode='bicubic', align_corners=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = torch.from_numpy(train_images).permute(0,3,1,2)\n",
    "y_tr = y_tr.float()\n",
    "y_te = torch.from_numpy(test_images).permute(0,3,1,2)\n",
    "y_te = y_te.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_images\n",
    "del test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = bicubicDownsample(y_tr)\n",
    "x_tr = x_tr.float()\n",
    "x_te = bicubicDownsample(y_te)\n",
    "x_te = x_te.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = y_tr.contiguous()\n",
    "y_te = y_te.contiguous()\n",
    "print(x_tr.is_contiguous())\n",
    "print(x_te.is_contiguous())\n",
    "print(y_tr.is_contiguous())\n",
    "print(y_te.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.4438008788229047, 0.43282444892005567, 0.39990855960960575],\n",
    "                                     std=[0.26915865140736284, 0.25496531678120016, 0.2794830545396813])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom training dataset\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.x = x_tr\n",
    "        self.y = y_tr\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            return self.transform(self.x[index]), self.transform(self.y[index])\n",
    "        else:\n",
    "            return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "# Creating custom testing dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.x = x_te\n",
    "        self.y = y_te\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            return self.transform(self.x[index]), self.transform(self.y[index])\n",
    "        else:\n",
    "            return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset()\n",
    "test_dataset = TestDataset()\n",
    "\n",
    "# Implementing train loader to split the data into batches\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, # data reshuffled at every epoch\n",
    "                          num_workers=2) # Use several subprocesses to load the data\n",
    "\n",
    "# Implementing train loader to split the data into batches\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, # data reshuffled at every epoch\n",
    "                          num_workers=2) # Use several subprocesses to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "n_samples = len(train_dataset)\n",
    "n_iterations = math.ceil(n_samples/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Channel Attention (CA) Layer\n",
    "class CALayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super().__init__()\n",
    "        # global average pooling: feature --> point\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        # feature channel downscale and upscale --> channel weight\n",
    "        self.conv_du = nn.Sequential(\n",
    "                nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "        #torch.nn.init.kaiming_uniform_(self.conv_du[0].weight, nonlinearity='relu')\n",
    "        #torch.nn.init.kaiming_uniform_(self.conv_du[2].weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.conv_du(y)\n",
    "        return x * y\n",
    "    \n",
    "## Residual Channel Attention Block (RCAB)\n",
    "class RCAB(nn.Module):\n",
    "    def __init__(self, n_feat, kernel_size=3, reduction=16,bias=False, bn=False, act=nn.ReLU(True), res_scale=1):\n",
    "        super().__init__()\n",
    "        modules_body = []\n",
    "        for i in range(2):\n",
    "            modules_body.append(nn.Conv2d(n_feat, n_feat, kernel_size, bias=bias, padding=1))\n",
    "            if bn: modules_body.append(nn.InstanceNorm2d(n_feat, affine=True))\n",
    "            if i == 0: modules_body.append(act)\n",
    "        modules_body.append(CALayer(n_feat, reduction))\n",
    "        self.body = nn.Sequential(*modules_body)\n",
    "        self.res_scale = res_scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        res += x\n",
    "        return res\n",
    "    \n",
    "## Residual Group (RG)\n",
    "class ResidualGroup(nn.Module):\n",
    "    def __init__(self, n_feat, n_resblocks, inorm, kernel_size=3, reduction=16, act=nn.ReLU(True), res_scale=1):\n",
    "        super().__init__()\n",
    "        modules_body = []\n",
    "        modules_body = [\n",
    "            RCAB(\n",
    "                n_feat, kernel_size, reduction, bias=True, bn=inorm, act=nn.ReLU(True), res_scale=1) \\\n",
    "            for _ in range(n_resblocks)]\n",
    "        modules_body.append(nn.Conv2d(n_feat, n_feat, kernel_size, padding=1))\n",
    "        #torch.nn.init.kaiming_uniform_(modules_body[-1].weight, nonlinearity='relu')\n",
    "        self.body = nn.Sequential(*modules_body)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        res += x\n",
    "        return res\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels*2, kernel_size=1, stride=1, bias=True),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.RG1 = ResidualGroup(n_feat=in_channels, n_resblocks=5, inorm=True)\n",
    "        self.reduce_channels = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=True)\n",
    "        self.RG2 = ResidualGroup(n_feat=out_channels, n_resblocks=5, inorm=True)\n",
    "        torch.nn.init.kaiming_uniform_(self.upsample[0].weight, nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_uniform_(self.reduce_channels.weight, nonlinearity='relu') \n",
    "            \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upsample(x1)\n",
    "        x = torch.cat((x2,x1), dim=1)\n",
    "        x = self.RG1(x)\n",
    "        x = self.reduce_channels(x)\n",
    "        x = self.RG2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class DecoderLayerNoReduce(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels*2, kernel_size=1, stride=1, bias=True),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, inplace=True)) \n",
    "        self.RG1 = ResidualGroup(n_feat=in_channels, n_resblocks=10, inorm=True)\n",
    "        torch.nn.init.kaiming_uniform_(self.upsample[0].weight, nonlinearity='relu')\n",
    "            \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upsample(x1)\n",
    "        x = torch.cat((x2,x1), dim=1)\n",
    "        x = self.RG1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FinalDecoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        n_resgroups = 10\n",
    "        modules_body = [\n",
    "            ResidualGroup(\n",
    "                n_feat=in_channels, n_resblocks=20, inorm=False) \\\n",
    "            for _ in range(n_resgroups)]\n",
    "        modules_body.append(nn.Conv2d(64, 64, kernel_size=3, padding=1))\n",
    "        torch.nn.init.kaiming_uniform_(modules_body[-1].weight, nonlinearity='relu')\n",
    "        self.body=nn.Sequential(*modules_body)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        res += x\n",
    "        return res\n",
    "    \n",
    "class Upsampler(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels*4, kernel_size=1, stride=1, bias=True),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "        torch.nn.init.kaiming_uniform_(self.upsample[0].weight, nonlinearity='relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class ResUNet(nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = torchvision.models.resnet34(pretrained=True, progress=False)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "        \n",
    "        # Encoder path\n",
    "        self.in_layer = nn.Sequential(*self.base_layers[0:3])\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[4])\n",
    "        self.layer2 = nn.Sequential(*self.base_layers[5])\n",
    "        self.layer3 = nn.Sequential(*self.base_layers[6])\n",
    "        \n",
    "        # Cross path\n",
    "        self.cross = nn.Conv2d(3, 32 ,kernel_size=1, bias=True)\n",
    "        self.origional_up = nn.Conv2d(3, 64 ,kernel_size=3, padding=1, bias=True)\n",
    "        \n",
    "        # Decoder path\n",
    "        self.up1 = DecoderLayer(256, 128)\n",
    "        self.up2 = DecoderLayer(128, 64)\n",
    "        self.up3 = DecoderLayerNoReduce(64)\n",
    "        self.up4 = FinalDecoderLayer(64)\n",
    "        self.upsample = Upsampler(64)\n",
    "        self.final_conv = nn.Conv2d(64, 3, kernel_size=9, padding=4)\n",
    "        torch.nn.init.kaiming_uniform_(self.cross.weight, nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_uniform_(self.origional_up.weight, nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_uniform_(self.final_conv.weight, nonlinearity='relu')\n",
    "      \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Encoder path\n",
    "        x_inp = x\n",
    "        x_in = self.in_layer(x)\n",
    "        x_l1 = self.layer1(x_in)\n",
    "        x_l2 = self.layer2(x_l1)\n",
    "        x_l3 = self.layer3(x_l2)\n",
    "        \n",
    "        # Decoder path\n",
    "        x = self.up1(x_l3, x_l2)\n",
    "        x = self.up2(x, x_l1)\n",
    "        x_origional_up = self.origional_up(x_inp)\n",
    "        x_inp = self.cross(x_inp)\n",
    "        x = self.up3(x, x_inp) \n",
    "        x = self.up4(x)\n",
    "        x = x + x_origional_up\n",
    "        x = self.upsample(x)\n",
    "        x = self.final_conv(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Loss Function (Perceptual Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = torchvision.models.vgg16(pretrained=True, progress=False)\n",
    "        features = model.features\n",
    "        self.relu1_2 = nn.Sequential()\n",
    "        self.relu2_2 = nn.Sequential()\n",
    "        self.relu3_3 = nn.Sequential()\n",
    "        self.relu4_3 = nn.Sequential()\n",
    "        for i in range(4):\n",
    "            self.relu1_2.add_module(name=\"relu1_2_\"+str(i+1), module=features[i])\n",
    "        for i in range(4, 9):\n",
    "            self.relu2_2.add_module(name=\"relu2_2_\"+str(i-3), module=features[i])\n",
    "        for i in range(9, 16):\n",
    "            self.relu3_3.add_module(name=\"relu3_3_\"+str(i-8), module=features[i])\n",
    "        for i in range(16, 23):\n",
    "            self.relu4_3.add_module(name=\"relu4_3_\"+str(i-15), module=features[i])      \n",
    "        # Setting requires_grad=False to fix the perceptual loss model parameters \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out_relu1_2 = self.relu1_2(x)\n",
    "        out_relu2_2 = self.relu2_2(out_relu1_2)\n",
    "        out_relu3_3 = self.relu3_3(out_relu2_2)\n",
    "        out_relu4_3 = self.relu4_3(out_relu3_3)\n",
    "        return out_relu1_2, out_relu2_2, out_relu3_3, out_relu4_3\n",
    "    \n",
    "# Function to calculate Gram matrix\n",
    "def gram(x):\n",
    "    (N, C, H, W) = x.shape\n",
    "    psy = x.view(N, C, H*W)\n",
    "    psy_T = psy.transpose(1, 2)\n",
    "    G = torch.bmm(psy, psy_T) / (C*H*W)  # Should we divide by N here? Or does batch matric multiplication do that on it's own? \n",
    "    return G\n",
    "\n",
    "def TVR(x, TV_WEIGHT=1e-6):\n",
    "    diff_i = torch.sum(torch.abs(x[:, :, :, 1:] - x[:, :, :, :-1]))\n",
    "    diff_j = torch.sum(torch.abs(x[:, :, 1:, :] - x[:, :, :-1, :]))\n",
    "    tv_loss = TV_WEIGHT*(diff_i + diff_j)\n",
    "    return tv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGGLoss = VGGPerceptualLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PerceptualLoss(x, y, STYLE_WEIGHT=1e-4, CONTENT_WEIGHT=1e-1, PIXEL_WEIGHT=1):\n",
    "    \n",
    "    x_features = VGGLoss(x)\n",
    "    y_features = VGGLoss(y)\n",
    "    \n",
    "    # Calculating per-pixel loss\n",
    "    C = y.shape[1]\n",
    "    H = y.shape[2]\n",
    "    W = y.shape[3]\n",
    "    pixel_loss =  F.mse_loss(x, y, reduction='sum') / (C*H*W)\n",
    "    #print(\"pixel loss= \",pixel_loss)\n",
    "    \n",
    "    # Calculating Total variation regularization value\n",
    "    tvr_loss = TVR(x)\n",
    "    #print(\"tvr loss= \", tvr_loss)\n",
    "    \n",
    "    # Calculating content loss\n",
    "    #weights = [0.25, 0.25, 0.25]\n",
    "    content_loss = 0.0\n",
    "    for i in range(2,4):\n",
    "        C = y_features[i].shape[1]\n",
    "        H = y_features[i].shape[2]\n",
    "        W = y_features[i].shape[3]\n",
    "        content_loss += (F.l1_loss(y_features[i], x_features[i], reduction='sum') / (C*H*W))\n",
    "        #print('c',content_loss)\n",
    "    #print(\"content loss= \", content_loss)\n",
    "    '''\n",
    "    # Calculating content loss\n",
    "    C = y_features[2].shape[1]\n",
    "    H = y_features[2].shape[2]\n",
    "    W = y_features[2].shape[3]\n",
    "    content_loss = F.l1_loss(x_features[2], y_features[2], reduction='sum') / (C*H*W)\n",
    "    #print(content_loss)\n",
    "    '''\n",
    "    # Calculating Style loss\n",
    "    style_loss = 0.0\n",
    "    for i in range(4):\n",
    "        C = y_features[i].shape[1]\n",
    "        H = y_features[i].shape[2]\n",
    "        W = y_features[i].shape[3]\n",
    "        style_loss += F.l1_loss(gram(x_features[i]), gram(y_features[i]), reduction='sum')\n",
    "        #print('s ',style_loss)\n",
    "    #print(\"style loss= \", style_loss)\n",
    "    total_loss = STYLE_WEIGHT*style_loss + CONTENT_WEIGHT*content_loss + PIXEL_WEIGHT*pixel_loss + tvr_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "example = iter(train_loader)\n",
    "example_data, example_target = example.next()\n",
    "example_data = F.interpolate(example_data, scale_factor=4, mode='bicubic', align_corners=False)\n",
    "loss = PerceptualLoss(example_data.to(device), example_target.to(device))\n",
    "del example\n",
    "del example_data\n",
    "del example_target\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing checkpoints\n",
    "def save_checkpoint_best(epoch, model):\n",
    "    print(\"Saving best model\")\n",
    "    PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/BestModel/best_model_\"+str(epoch)+\".pt\"\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, loss):  # Saving model in a way so we can load and start training again\n",
    "    PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/Models/model_\"+str(epoch)+\".pt\"\n",
    "    print(\"Saving model\")\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': loss,\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model\n",
    "tr_loss_log = []\n",
    "val_loss_log = []\n",
    "model = ResUNet(3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, layer in model._modules.items():\n",
    "    if name in ['in_layer', 'layer1', 'layer2', 'layer3', 'layer4']:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=[]\n",
    "decoder=[]\n",
    "for name, layer in model._modules.items():\n",
    "    if name in ['in_layer', 'layer1', 'layer2', 'layer3']:\n",
    "        print(name)\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "            encoder.append(param)\n",
    "    elif name in ['cross', 'up1', 'up2', 'up3', 'up4', 'upsample', 'final_conv']:\n",
    "        print(name)\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "            decoder.append(param)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "                {'params': decoder},\n",
    "                {'params': encoder, 'lr': 0.00005}\n",
    "            ], lr=0.0001)\n",
    "\n",
    "criterion = nn.L1Loss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = iter(train_loader)\n",
    "example_data, example_target = example.next()\n",
    "writer.add_graph(model, example_data.to(device))\n",
    "writer.close()\n",
    "del example\n",
    "del example_data\n",
    "del example_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_model():\n",
    "    least_val_loss = math.inf\n",
    "    C = 3\n",
    "    H = 128.0\n",
    "    W = 128.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        if((epoch+1)%25 == 0 and epoch!=99):\n",
    "            print('Learning Rates reduced')\n",
    "            print('New Learning Rates')\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = g['lr']/2\n",
    "                print(g['lr'])\n",
    "                \n",
    "        beg_time = time.time() #To calculate time taken for each epoch\n",
    "\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            # Will run for 1000 iterations per epoch\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            out = model(x)\n",
    "            #Calculating loss\n",
    "            loss = criterion(out, y)/(C*H*W)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update gradients\n",
    "            optimizer.step()\n",
    "            # Get training loss\n",
    "            train_loss += loss.item()\n",
    "        tr_loss_log.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(test_loader):\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                out = model(x)\n",
    "                #Calculating loss\n",
    "                loss = criterion(out, y)/(C*H*W)\n",
    "                # Get validation loss\n",
    "                val_loss += loss.item()\n",
    "            val_loss_log.append(val_loss)\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        # Saving checkpoints\n",
    "        #save_checkpoint(epoch+1, model, optimizer, val_loss)\n",
    "        if(val_loss < least_val_loss):\n",
    "            save_checkpoint_best(epoch+1, model)\n",
    "            least_val_loss = val_loss\n",
    "\n",
    "        end_time = time.time()\n",
    "        print('Epoch: {:.0f}/{:.0f}, Time: {:.0f}m {:.0f}s, Train_Loss: {:.4f}, Val_loss: {:.4f}'.format(\n",
    "              epoch+1, EPOCHS, (end_time-beg_time)//60, (end_time-beg_time)%60, train_loss, val_loss))\n",
    "        writer.add_scalar('Training_loss', train_loss, epoch+1)\n",
    "        writer.add_scalar('Validation_loss', val_loss, epoch+1)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()   #prev best ep 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=runs/rcan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the final model\n",
    "PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/FinalModel/rcan/64-128-full-l1-patches-100ep.pt\"\n",
    "print(\"Saving final model\")\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving just model\n",
    "PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/JustModels/final_trained_model_inorm128.pt\"\n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "model = ResUNet(3).to(device)\n",
    "model.load_state_dict(torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/FinalModel/36433/model-128-256.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the final model (fill in the final model epoch number)\n",
    "loaded_final_model = ResUNet(3).to(device)\n",
    "checkpoint = torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/FinalModel/modified-res-dense-inorm-big128.pt\")\n",
    "loaded_final_model.load_state_dict(checkpoint)\n",
    "#loaded_final_model.eval()\n",
    "model = loaded_final_model\n",
    "del loaded_final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the best model (fill in the best model epoch number)\n",
    "del model\n",
    "model = ResUNet(3).to(device)\n",
    "model.load_state_dict(torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/BestModel/best_model_74.pt\")) #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a model with desired epoch number\n",
    "loaded_model = ResUNet(3).to(device)\n",
    "checkpoint = torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/Models/model_8.pt\")\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model.eval()\n",
    "model = loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=[0.4438008788229047, 0.43282444892005567, 0.39990855960960575]\n",
    "std=[0.26915865140736284, 0.25496531678120016, 0.2794830545396813]\n",
    "mean = torch.FloatTensor(mean).view(3,1,1)\n",
    "std = torch.FloatTensor(std).view(3,1,1)\n",
    "def unorm(data):\n",
    "    img = data * std + mean\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "example = iter(test_loader)\n",
    "example_data, example_target = example.next()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "# Downsampled\n",
    "plt.subplot(2,2,1)\n",
    "plt.title('Downsampled')\n",
    "down = example_data[0]\n",
    "plt.imshow((down).permute(1,2,0).numpy())\n",
    "# Bi-linear upsampled\n",
    "out_1 = F.interpolate(example_data, scale_factor=2, mode='bicubic', align_corners=True)\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('Bi-linear upsampled')\n",
    "bi = out_1[0]\n",
    "plt.imshow((bi).permute(1,2,0).numpy())\n",
    "# Model prediction\n",
    "out_2 = model(example_data.to(device))\n",
    "plt.subplot(2,2,3)\n",
    "plt.title('Prediction')\n",
    "pred = out_2[0].cpu().detach()\n",
    "plt.imshow((pred).permute(1,2,0).numpy())\n",
    "# Ground truth\n",
    "plt.subplot(2,2,4)\n",
    "plt.title('Ground truth')\n",
    "gt = example_target[0]\n",
    "plt.imshow((gt).permute(1,2,0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from san\n",
    "#from skimage.metrics import structural_similarity as ssim\n",
    "def calc_psnr(sr, hr, scale, rgb_range, benchmark=False):\n",
    "    diff = (sr - hr).data.div(rgb_range)\n",
    "    shave = scale\n",
    "    if diff.size(1) > 1:\n",
    "        convert = diff.new(1, 3, 1, 1)\n",
    "        convert[0, 0, 0, 0] = 65.738\n",
    "        convert[0, 1, 0, 0] = 129.057\n",
    "        convert[0, 2, 0, 0] = 25.064\n",
    "        diff.mul_(convert).div_(256)\n",
    "        diff = diff.sum(dim=1, keepdim=True)\n",
    "    '''\n",
    "    if benchmark:\n",
    "        shave = scale\n",
    "        if diff.size(1) > 1:\n",
    "            convert = diff.new(1, 3, 1, 1)\n",
    "            convert[0, 0, 0, 0] = 65.738\n",
    "            convert[0, 1, 0, 0] = 129.057\n",
    "            convert[0, 2, 0, 0] = 25.064\n",
    "            diff.mul_(convert).div_(256)\n",
    "            diff = diff.sum(dim=1, keepdim=True)\n",
    "    else:\n",
    "        shave = scale + 6\n",
    "    '''\n",
    "    shave = scale + 6\n",
    "    valid = diff[:, :, shave:-shave, shave:-shave]\n",
    "    mse = valid.pow(2).mean()\n",
    "\n",
    "    return -10 * math.log10(mse)\n",
    "\n",
    "def quantize(img):\n",
    "    rgb_range = float(img.max()) - float(img.min())\n",
    "    pixel_range = 255 / rgb_range\n",
    "    return img.mul(pixel_range).clamp(0, 255).round().div(pixel_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all values for set 5\n",
    "import matplotlib\n",
    "directory = '/workspace/data/Dhruv/pytorch/Set5'    # use and f!='woman.png' and f!='head.png'\n",
    "#directory = '/workspace/data/Dhruv/pytorch/Set14/Set14'   # use and f!='comic.png' and f!='zebra.png' and f!='ppt3.png'\n",
    "img=[]\n",
    "names=[]\n",
    "for f in os.listdir(directory):\n",
    "    if(f.endswith('.png') and f!='woman.png' and f!='head.png'):\n",
    "        img.append(skimage.io.imread(directory+'/'+f))\n",
    "        names.append(f)\n",
    "mypsnr=0\n",
    "myssim=0\n",
    "for i in range(len(img)):\n",
    "    img_in = img[i].astype(np.float32)/255.0\n",
    "    img_in = np.expand_dims(img_in, 0)\n",
    "    img_in = torch.from_numpy(img_in).permute(0,3,1,2)\n",
    "    img_down = bicubicDownsample(img_in)\n",
    "    #img_down = (img_down - mean)/std\n",
    "    print(names[i])\n",
    "    out = model(img_down.to(device))\n",
    "    out = (out.cpu().detach())\n",
    "    #out_img = out[0].cpu().detach().permute(1,2,0).numpy()\n",
    "    #img_gt = (img[i]*255.0).astype(np.uint8).clip(0,255)\n",
    "    #out_img = (out_img*255.0).astype(np.uint8).clip(0,255)\n",
    "    psnr1 = calc_psnr(quantize(img_in[0].unsqueeze(0)), quantize(out), 2, float(quantize(out).max()) - float(quantize(out).min()))\n",
    "    print(out.shape)\n",
    "    print(img_in[0].unsqueeze(0).shape)\n",
    "    plt.imshow(quantize(out[0]).detach().permute(1,2,0).numpy())\n",
    "    plt.show()\n",
    "    mypsnr+=psnr1\n",
    "    #myssim+=ssim1\n",
    "\n",
    "print(mypsnr/len(img))\n",
    "print(myssim/len(img))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model1.eval()\n",
    "#example = iter(test_loader)\n",
    "#example_data, example_target = example.next()\n",
    "\n",
    "# old is only with 2 in perceptual, new is with 2 and 3 in perceptual\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(1,2,1)\n",
    "out_2 = model(example_data.to(device))\n",
    "plt.title('Prediction model')\n",
    "plt.imshow(out_2[15].cpu().detach().permute(1,2,0))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Prediction model1')\n",
    "out_2 = model1(example_data.to(device))\n",
    "plt.imshow(out_2[15].cpu().detach().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparision\n",
    "import matplotlib\n",
    "directory = 'compare/butterfly.png'\n",
    "img = skimage.io.imread(directory)\n",
    "img = img/255.0\n",
    "#img = skimage.transform.resize(img,(64,64))\n",
    "img = np.asarray(img)\n",
    "img = np.expand_dims(img, 0)\n",
    "img = torch.from_numpy(img).permute(0,3,1,2).float()\n",
    "out = model(img.to(device))\n",
    "out_img = out[0].cpu().detach().permute(1,2,0).numpy()\n",
    "plt.imshow(out_img)\n",
    "#matplotlib.image.imsave('compare/my_butterfly_4x.png', np.clip(out_img,0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature maps\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, _ = test_dataset[190]\n",
    "plt.imshow(data.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find activations for all the layers of the model\n",
    "for name, layer in model._modules.items():\n",
    "  layer.register_forward_hook(get_activation(name))\n",
    "data, _ = test_dataset[190]\n",
    "data.unsqueeze_(0)\n",
    "output = model(data.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = activation['final_conv'].squeeze().cpu()\n",
    "for idx in range(act.size(0)):\n",
    "    plt.imshow(act[idx])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.upconv4.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
