{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import sklearn.feature_extraction\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "%load_ext tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from fastai.layers import PixelShuffle_ICNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/workspace/data/Dhruv/pytorch/SuperResolution/tiny-imagenet-200'\n",
    "writer = SummaryWriter('runs/resunet34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.zeros((100000-1821, 64, 64, 3)) # 1821 images have dimension (64x64) and not (64x64x3)\n",
    "# Total good train images = 98179\n",
    "i = 0\n",
    "for f1 in os.listdir(dir + '/' + 'train'):\n",
    "    for f2 in os.listdir(dir + '/' + 'train/' + f1 + '/images/'):\n",
    "        if(f2.endswith(\".JPEG\")):\n",
    "            img = skimage.io.imread(dir + '/' + 'train/' + f1 + '/images/' + f2)\n",
    "            if img.ndim ==3:\n",
    "                train_images[i] = img\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_np = np.zeros((98179, 64, 64, 3))\n",
    "for i in range(98179):\n",
    "    train_images_np[i] = train_images[i]/255\n",
    "train_images = train_images_np\n",
    "del train_images_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = np.zeros((9832, 64, 64, 3)) # 9832 validation images\n",
    "i = 0\n",
    "for f in os.listdir(dir + '/' + 'val/images'):\n",
    "    if(f.endswith(\".JPEG\")):\n",
    "        img = skimage.io.imread(dir + '/' + 'val/images/' + f)\n",
    "        if img.ndim ==3:\n",
    "            test_images[i] = img\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_np = np.zeros((9832, 64, 64, 3))\n",
    "for i in range(9832):\n",
    "    test_images_np[i] = test_images[i]/255\n",
    "test_images = test_images_np\n",
    "del test_images_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bicubicDownsample(images, scale_factor=0.5):\n",
    "    out = torch.nn.functional.interpolate(images, scale_factor=scale_factor, mode='bicubic', align_corners=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = torch.from_numpy(train_images).permute(0,3,1,2)\n",
    "y_tr = y_tr.float()\n",
    "y_te = torch.from_numpy(test_images).permute(0,3,1,2)\n",
    "y_te = y_te.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_images\n",
    "del test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = bicubicDownsample(y_tr)\n",
    "x_tr = x_tr.float()\n",
    "x_te = bicubicDownsample(y_te)\n",
    "x_te = x_te.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = y_tr.contiguous()\n",
    "y_te = y_te.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_tr.is_contiguous())\n",
    "print(x_te.is_contiguous())\n",
    "print(y_tr.is_contiguous())\n",
    "print(y_te.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom training dataset\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.x = x_tr\n",
    "        self.y = y_tr\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            return self.transform(self.x[index]), self.y[index]\n",
    "        else:\n",
    "            return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "# Creating custom testing dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.x = x_te\n",
    "        self.y = y_te\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            return self.transform(self.x[index]), self.y[index]\n",
    "        else:\n",
    "            return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset()\n",
    "test_dataset = TestDataset()\n",
    "\n",
    "# Implementing train loader to split the data into batches\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, # data reshuffled at every epoch\n",
    "                          num_workers=2) # Use several subprocesses to load the data\n",
    "\n",
    "# Implementing train loader to split the data into batches\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, # data reshuffled at every epoch\n",
    "                          num_workers=2) # Use several subprocesses to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "n_samples = len(train_dataset)\n",
    "n_iterations = math.ceil(n_samples/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size = 1) # Skip connection\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        torch.nn.init.xavier_uniform_(self.skip.weight)\n",
    "        self.skip.bias.data.fill_(0.1)\n",
    "        \n",
    "        for i in [0,3]:\n",
    "            torch.nn.init.xavier_uniform_(self.double_conv[i].weight)\n",
    "            self.double_conv[i].bias.data.fill_(0.1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip_x = self.skip(x)\n",
    "        conv_x = self.double_conv(x)\n",
    "        added_x = skip_x + conv_x  # Element-wise addition of skip connection filters and residual filters\n",
    "        return F.relu_(added_x) # Inplace functional version of relu\n",
    "\n",
    "\n",
    "class PsUpsample(nn.Module): # Upsampling using pixel shuffle\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upsample = PixelShuffle_ICNR(in_channels, in_channels//2, scale=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upsample(x1)\n",
    "        x = torch.cat((x2, x1), dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResUNet(nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = torchvision.models.resnet34(pretrained=True, progress=False)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "        \n",
    "        # Encoder path\n",
    "        self.in_layer1 = self.base_layers[0]\n",
    "        self.in_layer2 = nn.Sequential(*self.base_layers[1:4])\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[4])\n",
    "        self.layer2 = nn.Sequential(*self.base_layers[5])\n",
    "        self.layer3 = nn.Sequential(*self.base_layers[6])\n",
    "        self.layer4 = nn.Sequential(*self.base_layers[7])\n",
    "        \n",
    "        # Cross path\n",
    "        self.down_in1 = nn.Conv2d(64, 32 ,kernel_size=1)\n",
    "        self.down_up = nn.Conv2d(3, 16, kernel_size=1)\n",
    "        \n",
    "        # Decoder path\n",
    "        self.up1 = PsUpsample(512, 256)\n",
    "        self.up2 = PsUpsample(256, 128)\n",
    "        self.up3 = PsUpsample(128, 64)\n",
    "        self.up4 = PsUpsample(64, 32)\n",
    "        self.up5 = PsUpsample(32, 16)\n",
    "        \n",
    "        self.out_layer = DoubleConv(16,3)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Encoder path\n",
    "        x_up = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        x_in1 = self.in_layer1(x_up)\n",
    "        x_in2 = self.in_layer2(x_in1) # This is of same size as x_l1 so not used\n",
    "        x_l1 = self.layer1(x_in2)\n",
    "        x_l2 = self.layer2(x_l1)\n",
    "        x_l3 = self.layer3(x_l2)\n",
    "        x_l4 = self.layer4(x_l3)\n",
    "        \n",
    "        # Decoder path\n",
    "        x = self.up1(x_l4, x_l3)\n",
    "        x = self.up2(x, x_l2)\n",
    "        x = self.up3(x, x_l1)\n",
    "        x_in1 = self.down_in1(x_in1)\n",
    "        x = self.up4(x, x_in1)\n",
    "        x_up = self.down_up(x_up)\n",
    "        x = self.up5(x, x_up)\n",
    "        x = self.out_layer(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Loss Function (Perceptual Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = torchvision.models.vgg19(pretrained=True, progress=False)\n",
    "        features = model.features\n",
    "        self.relu2_2 = nn.Sequential()\n",
    "        for i in range(9):\n",
    "            self.relu2_2.add_module(name=\"relu2_2_\"+str(i+1), module=features[i])    \n",
    "        # Setting requires_grad=False to fix the perceptual loss model parameters \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out_relu2_2 = self.relu2_2(x)\n",
    "        return out_relu2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGGLoss = VGGPerceptualLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PerceptualLoss(x, y):\n",
    "    \n",
    "    x_features = VGGLoss(x)\n",
    "    y_features = VGGLoss(y)\n",
    "    \n",
    "    # Calculating feature loss\n",
    "    C = y_features.shape[1]\n",
    "    H = y_features.shape[2]\n",
    "    W = y_features.shape[3]\n",
    "    feature_loss = (F.l1_loss(y_features, x_features, reduction='sum') / (C*H*W)) + (F.l1_loss(x, y) / (C*H*W)) # Here assuming square of Euclidean Norm = MSE Loss\n",
    "    return feature_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing checkpoints\n",
    "def save_checkpoint_best(epoch, model):\n",
    "    print(\"Saving best model\")\n",
    "    PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/BestModel/best_model_\"+str(epoch)+\".pt\"\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, loss):  # Saving model in a way so we can load and start training again\n",
    "    PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/Models/model_\"+str(epoch)+\".pt\"\n",
    "    print(\"Saving model\")\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': loss,\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss_log = []\n",
    "val_loss_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResUNet(3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set requires_grad of parameters of base_model = False\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "#loss = PerceptualLoss used directly in training loop\n",
    "\n",
    "example = iter(train_loader)\n",
    "example_data, example_target = example.next()\n",
    "writer.add_graph(model, example_data.to(device))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=6, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_model():\n",
    "\n",
    "  least_val_loss = math.inf\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "      \n",
    "      beg_time = time.time() #To calculate time taken for each epoch\n",
    "      \n",
    "      train_loss = 0.0\n",
    "      val_loss = 0.0\n",
    "      \n",
    "      for i, (x, y) in enumerate(train_loader):\n",
    "          x = x.to(device)\n",
    "          y = y.to(device)\n",
    "          # Will run for 1000 iterations per epoch\n",
    "          optimizer.zero_grad()\n",
    "          # Forward pass\n",
    "          out = model(x)\n",
    "          #Calculating loss\n",
    "          loss = PerceptualLoss(out, y)\n",
    "          # Backward pass\n",
    "          loss.backward()\n",
    "          # Update gradients\n",
    "          optimizer.step()\n",
    "          # Get training loss\n",
    "          train_loss += loss.item()\n",
    "      tr_loss_log.append(train_loss)\n",
    "      \n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "          for i, (x, y) in enumerate(test_loader):\n",
    "              x = x.to(device)\n",
    "              y = y.to(device)\n",
    "              out = model(x)\n",
    "              #Calculating loss\n",
    "              loss = PerceptualLoss(out, y)\n",
    "              # Get validation loss\n",
    "              val_loss += loss.item()\n",
    "          val_loss_log.append(val_loss)\n",
    "      model.train()\n",
    "    \n",
    "      scheduler.step(val_loss)\n",
    "      \n",
    "      # Saving checkpoints\n",
    "      save_checkpoint(epoch+1, model, optimizer, val_loss)\n",
    "      if(val_loss < least_val_loss):\n",
    "          save_checkpoint_best(epoch+1, model)\n",
    "          least_val_loss = val_loss\n",
    "          \n",
    "      end_time = time.time()\n",
    "      print('Epoch: {:.0f}/{:.0f}, Time: {:.0f}m {:.0f}s, Train_Loss: {:.4f}, Val_loss: {:.4f}'.format(\n",
    "          epoch+1, EPOCHS, (end_time-beg_time)//60, (end_time-beg_time)%60, train_loss, val_loss))\n",
    "      writer.add_scalar('Training_loss', train_loss, epoch*n_iterations+i)\n",
    "      writer.add_scalar('Validation_loss', val_loss, epoch*n_iterations+i)\n",
    "      writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=runs/resunet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the final model\n",
    "PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/FinalModel/final_trained_model.pt\"\n",
    "print(\"Saving final model\")\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To load the best model (fill in the best model epoch number)\n",
    "loaded_best_model = ResUNet(3).to(device)\n",
    "checkpoint = torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/BestModel/best_model_15.pt\")\n",
    "loaded_best_model.load_state_dict(checkpoint)\n",
    "loaded_best_model.eval()\n",
    "model = loaded_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading a model with desired epoch number\n",
    "loaded_model = ResUNet(3).to(device)\n",
    "checkpoint = torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/Models/model_8.pt\")\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model.eval()\n",
    "model = loaded_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "example = iter(test_loader)\n",
    "example_data, example_target = example.next()\n",
    "plt.imshow(example_data[15].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(example_data.to(device))\n",
    "plt.imshow(out[15].cpu().detach().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(example_target[15].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature maps\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, _ = test_dataset[30]\n",
    "plt.imshow(data.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.in_layer2.register_forward_hook(get_activation('in_layer2'))\n",
    "data, _ = test_dataset[30]\n",
    "data.unsqueeze_(0)\n",
    "output = model(data.to(device))\n",
    "\n",
    "act = activation['in_layer2'].squeeze().cpu()\n",
    "#fig, axarr = plt.subplots(act.size(0))\n",
    "for idx in range(act.size(0)):\n",
    "    plt.imshow(act[idx])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
