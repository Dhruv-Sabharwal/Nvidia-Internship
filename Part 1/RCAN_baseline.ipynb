{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import sklearn.feature_extraction\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from skimage import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "%load_ext tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from fastai.layers import PixelShuffle_ICNR\n",
    "from skimage.transform import rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/rcan/64-128-rcan-baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/workspace/data/Dhruv/pytorch/SuperResolution/Data'\n",
    "train_images = np.zeros((800, 700, 700, 3))\n",
    "i = 0\n",
    "for f in os.listdir(dir + '/' + 'Train'):\n",
    "    if(f.endswith(\".png\")):\n",
    "        train_images[i] = (skimage.transform.resize(\n",
    "            io.imread(dir + '/Train/' + f),(700, 700), mode ='constant'))\n",
    "        i += 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/workspace/data/Dhruv/pytorch/SuperResolution/Flickr2K'\n",
    "train_images1 = np.zeros((2400, 512, 512, 3))\n",
    "test_images1 = np.zeros((250, 512, 512, 3))\n",
    "i = 0\n",
    "for f in os.listdir(dir):\n",
    "    if(f.endswith(\".png\")):\n",
    "        if(i<2400):\n",
    "            train_images1[i] = (skimage.transform.resize(\n",
    "                skimage.io.imread(dir + '/' + f),(512,512), mode ='constant'))\n",
    "            i += 1\n",
    "        elif(i>=2400):\n",
    "            test_images1[i-2400] = (skimage.transform.resize(\n",
    "            skimage.io.imread(dir + '/' + f),(512,512), mode ='constant'))\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract 64x64 patches from the images. 20 patches from each image.\n",
    "def patchExtract(images, patch_size=(128, 128), max_patches=8):\n",
    "    pe = sklearn.feature_extraction.image.PatchExtractor(patch_size=patch_size, max_patches = max_patches)\n",
    "    pe_fit = pe.fit(images)\n",
    "    pe_trans = pe.transform(images)\n",
    "    return pe_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/workspace/data/Dhruv/pytorch/SuperResolution/Data'\n",
    "test_images = np.zeros((100, 700, 700, 3))\n",
    "i = 0\n",
    "for f in os.listdir(dir + '/' + 'Validation'):\n",
    "    if(f.endswith(\".png\")):\n",
    "        test_images[i] = (skimage.transform.resize(\n",
    "            io.imread(dir + '/Validation/' + f),(700, 700), mode ='constant'))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_images = np.concatenate((train_images, train_images1), axis=0)\n",
    "#test_images = np.concatenate((test_images, test_images1), axis=0)\n",
    "train_images = patchExtract(train_images)\n",
    "test_images = patchExtract(test_images)\n",
    "#np.random.shuffle(train_images)\n",
    "#np.random.shuffle(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bicubicDownsample(images, scale_factor=0.5):\n",
    "    out = torch.nn.functional.interpolate(images, scale_factor=scale_factor, mode='bicubic', align_corners=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = torch.from_numpy(train_images).permute(0,3,1,2)\n",
    "y_tr = y_tr.float()\n",
    "y_te = torch.from_numpy(test_images).permute(0,3,1,2)\n",
    "y_te = y_te.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_images\n",
    "del test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = bicubicDownsample(y_tr)\n",
    "x_tr = x_tr.float()\n",
    "x_te = bicubicDownsample(y_te)\n",
    "x_te = x_te.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = y_tr.contiguous()\n",
    "y_te = y_te.contiguous()\n",
    "print(x_tr.is_contiguous())\n",
    "print(x_te.is_contiguous())\n",
    "print(y_tr.is_contiguous())\n",
    "print(y_te.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.4438008788229047, 0.43282444892005567, 0.39990855960960575],\n",
    "                                     std=[0.26915865140736284, 0.25496531678120016, 0.2794830545396813])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom training dataset\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.x = x_tr\n",
    "        self.y = y_tr\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            return self.transform(self.x[index]), self.transform(self.y[index])\n",
    "        else:\n",
    "            return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "# Creating custom testing dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.x = x_te\n",
    "        self.y = y_te\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            return self.transform(self.x[index]), self.transform(self.y[index])\n",
    "        else:\n",
    "            return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset()\n",
    "test_dataset = TestDataset()\n",
    "\n",
    "# Implementing train loader to split the data into batches\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, # data reshuffled at every epoch\n",
    "                          num_workers=2) # Use several subprocesses to load the data\n",
    "\n",
    "# Implementing train loader to split the data into batches\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, # data reshuffled at every epoch\n",
    "                          num_workers=2) # Use several subprocesses to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "n_samples = len(train_dataset)\n",
    "n_iterations = math.ceil(n_samples/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def default_conv(in_channels, out_channels, kernel_size, bias=True):\n",
    "    return nn.Conv2d(\n",
    "        in_channels, out_channels, kernel_size,\n",
    "        padding=(kernel_size//2), bias=bias)\n",
    "\n",
    "class MeanShift(nn.Conv2d):\n",
    "    def __init__(self, rgb_range, rgb_mean, rgb_std, sign=-1):\n",
    "        super(MeanShift, self).__init__(3, 3, kernel_size=1)\n",
    "        std = torch.Tensor(rgb_std)\n",
    "        self.weight.data = torch.eye(3).view(3, 3, 1, 1)\n",
    "        self.weight.data.div_(std.view(3, 1, 1, 1))\n",
    "        self.bias.data = sign * rgb_range * torch.Tensor(rgb_mean)\n",
    "        self.bias.data.div_(std)\n",
    "        self.requires_grad = False\n",
    "\n",
    "class BasicBlock(nn.Sequential):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size, stride=1, bias=False,\n",
    "        bn=True, act=nn.ReLU(True)):\n",
    "\n",
    "        m = [nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size,\n",
    "            padding=(kernel_size//2), stride=stride, bias=bias)\n",
    "        ]\n",
    "        if bn: m.append(nn.BatchNorm2d(out_channels))\n",
    "        if act is not None: m.append(act)\n",
    "        super(BasicBlock, self).__init__(*m)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, conv, n_feat, kernel_size,\n",
    "        bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n",
    "\n",
    "        super(ResBlock, self).__init__()\n",
    "        m = []\n",
    "        for i in range(2):\n",
    "            m.append(conv(n_feat, n_feat, kernel_size, bias=bias))\n",
    "            if bn: m.append(nn.BatchNorm2d(n_feat))\n",
    "            if i == 0: m.append(act)\n",
    "\n",
    "        self.body = nn.Sequential(*m)\n",
    "        self.res_scale = res_scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.body(x).mul(self.res_scale)\n",
    "        res += x\n",
    "\n",
    "        return res\n",
    "\n",
    "class Upsampler(nn.Sequential):\n",
    "    def __init__(self, conv, scale, n_feat, bn=False, act=False, bias=True):\n",
    "\n",
    "        m = []\n",
    "        if (scale & (scale - 1)) == 0:    # Is scale = 2^n?\n",
    "            for _ in range(int(math.log(scale, 2))):\n",
    "                m.append(conv(n_feat, 4 * n_feat, 3, bias))\n",
    "                m.append(nn.PixelShuffle(2))\n",
    "                if bn: m.append(nn.BatchNorm2d(n_feat))\n",
    "                if act: m.append(act())\n",
    "        elif scale == 3:\n",
    "            m.append(conv(n_feat, 9 * n_feat, 3, bias))\n",
    "            m.append(nn.PixelShuffle(3))\n",
    "            if bn: m.append(nn.BatchNorm2d(n_feat))\n",
    "            if act: m.append(act())\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        super(Upsampler, self).__init__(*m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "## Channel Attention (CA) Layer\n",
    "class CALayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(CALayer, self).__init__()\n",
    "        # global average pooling: feature --> point\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        # feature channel downscale and upscale --> channel weight\n",
    "        self.conv_du = nn.Sequential(\n",
    "                nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=True),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.conv_du(y)\n",
    "        return x * y\n",
    "\n",
    "## Residual Channel Attention Block (RCAB)\n",
    "class RCAB(nn.Module):\n",
    "    def __init__(\n",
    "        self, conv, n_feat, kernel_size, reduction,\n",
    "        bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n",
    "\n",
    "        super(RCAB, self).__init__()\n",
    "        modules_body = []\n",
    "        for i in range(2):\n",
    "            modules_body.append(conv(n_feat, n_feat, kernel_size, bias=bias))\n",
    "            if bn: modules_body.append(nn.BatchNorm2d(n_feat))\n",
    "            if i == 0: modules_body.append(act)\n",
    "        modules_body.append(CALayer(n_feat, reduction))\n",
    "        self.body = nn.Sequential(*modules_body)\n",
    "        self.res_scale = res_scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        #res = self.body(x).mul(self.res_scale)\n",
    "        res += x\n",
    "        return res\n",
    "\n",
    "## Residual Group (RG)\n",
    "class ResidualGroup(nn.Module):\n",
    "    def __init__(self, conv, n_feat, kernel_size, reduction, act, res_scale, n_resblocks):\n",
    "        super(ResidualGroup, self).__init__()\n",
    "        modules_body = []\n",
    "        modules_body = [\n",
    "            RCAB(\n",
    "                conv, n_feat, kernel_size, reduction, bias=True, bn=False, act=nn.ReLU(True), res_scale=1) \\\n",
    "            for _ in range(n_resblocks)]\n",
    "        modules_body.append(conv(n_feat, n_feat, kernel_size))\n",
    "        self.body = nn.Sequential(*modules_body)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        res += x\n",
    "        return res\n",
    "\n",
    "## Residual Channel Attention Network (RCAN)\n",
    "class RCAN(nn.Module):\n",
    "    def __init__(self, conv=default_conv):\n",
    "        super(RCAN, self).__init__()\n",
    "        \n",
    "        n_resgroups = 10\n",
    "        n_resblocks = 20\n",
    "        n_feats = 64\n",
    "        kernel_size = 3\n",
    "        reduction = 16 \n",
    "        scale = 2\n",
    "        act = nn.ReLU(True)\n",
    "        \n",
    "        # RGB mean for DIV2K\n",
    "        '''\n",
    "        rgb_mean = (0.4488, 0.4371, 0.4040)\n",
    "        rgb_std = (1.0, 1.0, 1.0)\n",
    "        self.sub_mean = MeanShift(255, rgb_mean, rgb_std)\n",
    "        '''\n",
    "        \n",
    "        # define head module\n",
    "        modules_head = [conv(3, n_feats, kernel_size)]\n",
    "\n",
    "        # define body module\n",
    "        modules_body = [\n",
    "            ResidualGroup(\n",
    "                conv, n_feats, kernel_size, reduction, act=act, res_scale=1, n_resblocks=n_resblocks) \\\n",
    "            for _ in range(n_resgroups)]\n",
    "\n",
    "        modules_body.append(conv(n_feats, n_feats, kernel_size))\n",
    "\n",
    "        # define tail module\n",
    "        modules_tail = [\n",
    "            Upsampler(conv, scale, n_feats, act=False),\n",
    "            conv(n_feats, 3, kernel_size)]\n",
    "\n",
    "        #self.add_mean = MeanShift(255, rgb_mean, rgb_std, 1)\n",
    "\n",
    "        self.head = nn.Sequential(*modules_head)\n",
    "        self.body = nn.Sequential(*modules_body)\n",
    "        self.tail = nn.Sequential(*modules_tail)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.sub_mean(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        res = self.body(x)\n",
    "        res += x\n",
    "\n",
    "        x = self.tail(res)\n",
    "        #x = self.add_mean(x)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Loss Function (Perceptual Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = torchvision.models.vgg16(pretrained=True, progress=False)\n",
    "        features = model.features\n",
    "        self.relu1_2 = nn.Sequential()\n",
    "        self.relu2_2 = nn.Sequential()\n",
    "        self.relu3_3 = nn.Sequential()\n",
    "        self.relu4_3 = nn.Sequential()\n",
    "        for i in range(4):\n",
    "            self.relu1_2.add_module(name=\"relu1_2_\"+str(i+1), module=features[i])\n",
    "        for i in range(4, 9):\n",
    "            self.relu2_2.add_module(name=\"relu2_2_\"+str(i-3), module=features[i])\n",
    "        for i in range(9, 16):\n",
    "            self.relu3_3.add_module(name=\"relu3_3_\"+str(i-8), module=features[i])\n",
    "        for i in range(16, 23):\n",
    "            self.relu4_3.add_module(name=\"relu4_3_\"+str(i-15), module=features[i])      \n",
    "        # Setting requires_grad=False to fix the perceptual loss model parameters \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out_relu1_2 = self.relu1_2(x)\n",
    "        out_relu2_2 = self.relu2_2(out_relu1_2)\n",
    "        out_relu3_3 = self.relu3_3(out_relu2_2)\n",
    "        out_relu4_3 = self.relu4_3(out_relu3_3)\n",
    "        return out_relu1_2, out_relu2_2, out_relu3_3, out_relu4_3\n",
    "    \n",
    "# Function to calculate Gram matrix\n",
    "def gram(x):\n",
    "    (N, C, H, W) = x.shape\n",
    "    psy = x.view(N, C, H*W)\n",
    "    psy_T = psy.transpose(1, 2)\n",
    "    G = torch.bmm(psy, psy_T) / (C*H*W)  # Should we divide by N here? Or does batch matric multiplication do that on it's own? \n",
    "    return G\n",
    "\n",
    "def TVR(x, TV_WEIGHT=1e-6):\n",
    "    diff_i = torch.sum(torch.abs(x[:, :, :, 1:] - x[:, :, :, :-1]))\n",
    "    diff_j = torch.sum(torch.abs(x[:, :, 1:, :] - x[:, :, :-1, :]))\n",
    "    tv_loss = TV_WEIGHT*(diff_i + diff_j)\n",
    "    return tv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGGLoss = VGGPerceptualLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PerceptualLoss(x, y, STYLE_WEIGHT=1e-4, CONTENT_WEIGHT=1e-1, PIXEL_WEIGHT=1):\n",
    "    \n",
    "    x_features = VGGLoss(x)\n",
    "    y_features = VGGLoss(y)\n",
    "    \n",
    "    # Calculating per-pixel loss\n",
    "    C = y.shape[1]\n",
    "    H = y.shape[2]\n",
    "    W = y.shape[3]\n",
    "    pixel_loss =  F.mse_loss(x, y, reduction='sum') / (C*H*W)\n",
    "    #print(\"pixel loss= \",pixel_loss)\n",
    "    \n",
    "    # Calculating Total variation regularization value\n",
    "    tvr_loss = TVR(x)\n",
    "    #print(\"tvr loss= \", tvr_loss)\n",
    "    \n",
    "    # Calculating content loss\n",
    "    #weights = [0.25, 0.25, 0.25]\n",
    "    content_loss = 0.0\n",
    "    for i in range(2,4):\n",
    "        C = y_features[i].shape[1]\n",
    "        H = y_features[i].shape[2]\n",
    "        W = y_features[i].shape[3]\n",
    "        content_loss += (F.l1_loss(y_features[i], x_features[i], reduction='sum') / (C*H*W))\n",
    "        #print('c',content_loss)\n",
    "    #print(\"content loss= \", content_loss)\n",
    "    '''\n",
    "    # Calculating content loss\n",
    "    C = y_features[2].shape[1]\n",
    "    H = y_features[2].shape[2]\n",
    "    W = y_features[2].shape[3]\n",
    "    content_loss = F.l1_loss(x_features[2], y_features[2], reduction='sum') / (C*H*W)\n",
    "    #print(content_loss)\n",
    "    '''\n",
    "    # Calculating Style loss\n",
    "    style_loss = 0.0\n",
    "    for i in range(4):\n",
    "        C = y_features[i].shape[1]\n",
    "        H = y_features[i].shape[2]\n",
    "        W = y_features[i].shape[3]\n",
    "        style_loss += F.l1_loss(gram(x_features[i]), gram(y_features[i]), reduction='sum')\n",
    "        #print('s ',style_loss)\n",
    "    #print(\"style loss= \", style_loss)\n",
    "    total_loss = STYLE_WEIGHT*style_loss + CONTENT_WEIGHT*content_loss + PIXEL_WEIGHT*pixel_loss + tvr_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "example = iter(train_loader)\n",
    "example_data, example_target = example.next()\n",
    "example_data = F.interpolate(example_data, scale_factor=4, mode='bicubic', align_corners=False)\n",
    "loss = PerceptualLoss(example_data.to(device), example_target.to(device))\n",
    "del example\n",
    "del example_data\n",
    "del example_target\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing checkpoints\n",
    "def save_checkpoint_best(epoch, model):\n",
    "    print(\"Saving best model\")\n",
    "    PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/BestModel/best_model_\"+str(epoch)+\".pt\"\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, loss):  # Saving model in a way so we can load and start training again\n",
    "    PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/Models/model_\"+str(epoch)+\".pt\"\n",
    "    print(\"Saving model\")\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': loss,\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model\n",
    "tr_loss_log = []\n",
    "val_loss_log = []\n",
    "model = RCAN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, layer in model._modules.items():\n",
    "    if name in ['in_layer', 'layer1', 'layer2', 'layer3', 'layer4']:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=[]\n",
    "decoder=[]\n",
    "for name, layer in model._modules.items():\n",
    "    if name in ['in_layer', 'layer1', 'layer2', 'layer3']:\n",
    "        print(name)\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "            encoder.append(param)\n",
    "    elif name in ['cross', 'up1', 'up2', 'up3', 'up4', 'upsample', 'final_conv']:\n",
    "        print(name)\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "            decoder.append(param)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "                {'params': decoder},\n",
    "                {'params': encoder, 'lr': 0.00005}\n",
    "            ], lr=0.0001)\n",
    "\n",
    "criterion = nn.L1Loss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.L1Loss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = iter(train_loader)\n",
    "example_data, example_target = example.next()\n",
    "writer.add_graph(model, example_data.to(device))\n",
    "writer.close()\n",
    "del example\n",
    "del example_data\n",
    "del example_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_model():\n",
    "    least_val_loss = 14.7197\n",
    "    C = 3\n",
    "    H = 128.0\n",
    "    W = 128.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        if((epoch+1)%25 == 0 and epoch!=99):\n",
    "            print('Learning Rates reduced')\n",
    "            print('New Learning Rates')\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = g['lr']/2\n",
    "                print(g['lr'])\n",
    "        \n",
    "        beg_time = time.time() #To calculate time taken for each epoch\n",
    "\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            # Will run for 1000 iterations per epoch\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            out = model(x)\n",
    "            #Calculating loss\n",
    "            loss = criterion(out, y)/(C*H*W)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update gradients\n",
    "            optimizer.step()\n",
    "            # Get training loss\n",
    "            train_loss += loss.item()\n",
    "        tr_loss_log.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(test_loader):\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                out = model(x)\n",
    "                #Calculating loss\n",
    "                loss = criterion(out, y)/(C*H*W)\n",
    "                # Get validation loss\n",
    "                val_loss += loss.item()\n",
    "            val_loss_log.append(val_loss)\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        # Saving checkpoints\n",
    "        #save_checkpoint(epoch+1, model, optimizer, val_loss)\n",
    "        if(val_loss < least_val_loss):\n",
    "            save_checkpoint_best(epoch+1+100, model)\n",
    "            least_val_loss = val_loss\n",
    "\n",
    "        end_time = time.time()\n",
    "        print('Epoch: {:.0f}/{:.0f}, Time: {:.0f}m {:.0f}s, Train_Loss: {:.4f}, Val_loss: {:.4f}'.format(\n",
    "              epoch+1+100, EPOCHS, (end_time-beg_time)//60, (end_time-beg_time)%60, train_loss, val_loss))\n",
    "        writer.add_scalar('Training_loss', train_loss, epoch+1+100)\n",
    "        writer.add_scalar('Validation_loss', val_loss, epoch+1+100)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()  # prev best epoch 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=runs/rcan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the final model\n",
    "PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/FinalModel/rcan/64-128-baseline-patches-100ep.pt\"\n",
    "print(\"Saving final model\")\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving just model\n",
    "PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/JustModels/final_trained_model_inorm128.pt\"\n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "model = ResUNet(3).to(device)\n",
    "model.load_state_dict(torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/FinalModel/36433/model-128-256.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the final model (fill in the final model epoch number)\n",
    "loaded_final_model = ResUNet(3).to(device)\n",
    "checkpoint = torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/FinalModel/modified-res-dense-inorm-big128.pt\")\n",
    "loaded_final_model.load_state_dict(checkpoint)\n",
    "#loaded_final_model.eval()\n",
    "model = loaded_final_model\n",
    "del loaded_final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the best model (fill in the best model epoch number)\n",
    "#del model\n",
    "model = RCAN().to(device)\n",
    "model.load_state_dict(torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/BestModel/best_model_27.pt\")) #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a model with desired epoch number\n",
    "loaded_model = ResUNet(3).to(device)\n",
    "checkpoint = torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/Models/model_8.pt\")\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model.eval()\n",
    "model = loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=[0.4438008788229047, 0.43282444892005567, 0.39990855960960575]\n",
    "std=[0.26915865140736284, 0.25496531678120016, 0.2794830545396813]\n",
    "mean = torch.FloatTensor(mean).view(3,1,1)\n",
    "std = torch.FloatTensor(std).view(3,1,1)\n",
    "def unorm(data):\n",
    "    img = data * std + mean\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "example = iter(test_loader)\n",
    "example_data, example_target = example.next()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "# Downsampled\n",
    "plt.subplot(2,2,1)\n",
    "plt.title('Downsampled')\n",
    "down = example_data[0]\n",
    "plt.imshow((down).permute(1,2,0).numpy())\n",
    "# Bi-linear upsampled\n",
    "out_1 = F.interpolate(example_data, scale_factor=2, mode='bicubic', align_corners=True)\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('Bi-linear upsampled')\n",
    "bi = out_1[0]\n",
    "plt.imshow((bi).permute(1,2,0).numpy())\n",
    "# Model prediction\n",
    "out_2 = model(example_data.to(device))\n",
    "plt.subplot(2,2,3)\n",
    "plt.title('Prediction')\n",
    "pred = out_2[0].cpu().detach()\n",
    "plt.imshow((pred).permute(1,2,0).numpy())\n",
    "# Ground truth\n",
    "plt.subplot(2,2,4)\n",
    "plt.title('Ground truth')\n",
    "gt = example_target[0]\n",
    "plt.imshow((gt).permute(1,2,0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from san\n",
    "#from skimage.metrics import structural_similarity as ssim\n",
    "def calc_psnr(sr, hr, scale, rgb_range, benchmark=False):\n",
    "    diff = (sr - hr).data.div(rgb_range)\n",
    "    shave = scale\n",
    "    if diff.size(1) > 1:\n",
    "        convert = diff.new(1, 3, 1, 1)\n",
    "        convert[0, 0, 0, 0] = 65.738\n",
    "        convert[0, 1, 0, 0] = 129.057\n",
    "        convert[0, 2, 0, 0] = 25.064\n",
    "        diff.mul_(convert).div_(256)\n",
    "        diff = diff.sum(dim=1, keepdim=True)\n",
    "    '''\n",
    "    if benchmark:\n",
    "        shave = scale\n",
    "        if diff.size(1) > 1:\n",
    "            convert = diff.new(1, 3, 1, 1)\n",
    "            convert[0, 0, 0, 0] = 65.738\n",
    "            convert[0, 1, 0, 0] = 129.057\n",
    "            convert[0, 2, 0, 0] = 25.064\n",
    "            diff.mul_(convert).div_(256)\n",
    "            diff = diff.sum(dim=1, keepdim=True)\n",
    "    else:\n",
    "        shave = scale + 6\n",
    "    '''\n",
    "    shave = scale + 6\n",
    "    valid = diff[:, :, shave:-shave, shave:-shave]\n",
    "    mse = valid.pow(2).mean()\n",
    "\n",
    "    return -10 * math.log10(mse)\n",
    "\n",
    "def quantize(img):\n",
    "    rgb_range = float(img.max()) - float(img.min())\n",
    "    pixel_range = 255 / rgb_range\n",
    "    return img.mul(pixel_range).clamp(0, 255).round().div(pixel_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all values for set 5\n",
    "import matplotlib\n",
    "directory = '/workspace/data/Dhruv/pytorch/Set5'    # use and f!='woman.png' and f!='head.png'\n",
    "#directory = '/workspace/data/Dhruv/pytorch/Set14/Set14'   # use and f!='comic.png' and f!='zebra.png' and f!='ppt3.png'\n",
    "img=[]\n",
    "names=[]\n",
    "for f in os.listdir(directory):\n",
    "    if(f.endswith('.png') and f!='woman.png' and f!='head.png'):\n",
    "        img.append(skimage.io.imread(directory+'/'+f))\n",
    "        names.append(f)\n",
    "mypsnr=0\n",
    "myssim=0\n",
    "for i in range(len(img)):\n",
    "    img_in = img[i].astype(np.float32)/255.0\n",
    "    img_in = np.expand_dims(img_in, 0)\n",
    "    img_in = torch.from_numpy(img_in).permute(0,3,1,2)\n",
    "    img_down = bicubicDownsample(img_in)\n",
    "    #img_down = (img_down - mean)/std\n",
    "    print(names[i])\n",
    "    out = model(img_down.to(device))\n",
    "    out = out.cpu().detach()\n",
    "    #out_img = out[0].cpu().detach().permute(1,2,0).numpy()\n",
    "    #img_gt = (img[i]*255.0).astype(np.uint8).clip(0,255)\n",
    "    #out_img = (out_img*255.0).astype(np.uint8).clip(0,255)\n",
    "    psnr1 = calc_psnr(quantize(img_in[0].unsqueeze(0)), quantize(out), 2, float(quantize(out).max()) - float(quantize(out).min()))\n",
    "    print(out.shape)\n",
    "    print(img_in[0].unsqueeze(0).shape)\n",
    "    plt.imshow(quantize(out[0]).detach().permute(1,2,0).numpy())\n",
    "    plt.show()\n",
    "    mypsnr+=psnr1\n",
    "    #myssim+=ssim1\n",
    "\n",
    "print(mypsnr/len(img))\n",
    "print(myssim/len(img))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model1.eval()\n",
    "#example = iter(test_loader)\n",
    "#example_data, example_target = example.next()\n",
    "\n",
    "# old is only with 2 in perceptual, new is with 2 and 3 in perceptual\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(1,2,1)\n",
    "out_2 = model(example_data.to(device))\n",
    "plt.title('Prediction model')\n",
    "plt.imshow(out_2[15].cpu().detach().permute(1,2,0))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Prediction model1')\n",
    "out_2 = model1(example_data.to(device))\n",
    "plt.imshow(out_2[15].cpu().detach().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparision\n",
    "import matplotlib\n",
    "directory = 'compare/butterfly.png'\n",
    "img = skimage.io.imread(directory)\n",
    "img = img/255.0\n",
    "#img = skimage.transform.resize(img,(64,64))\n",
    "img = np.asarray(img)\n",
    "img = np.expand_dims(img, 0)\n",
    "img = torch.from_numpy(img).permute(0,3,1,2).float()\n",
    "out = model(img.to(device))\n",
    "out_img = out[0].cpu().detach().permute(1,2,0).numpy()\n",
    "plt.imshow(out_img)\n",
    "#matplotlib.image.imsave('compare/my_butterfly_4x.png', np.clip(out_img,0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature maps\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, _ = test_dataset[190]\n",
    "plt.imshow(data.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find activations for all the layers of the model\n",
    "for name, layer in model._modules.items():\n",
    "  layer.register_forward_hook(get_activation(name))\n",
    "data, _ = test_dataset[190]\n",
    "data.unsqueeze_(0)\n",
    "output = model(data.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = activation['final_conv'].squeeze().cpu()\n",
    "for idx in range(act.size(0)):\n",
    "    plt.imshow(act[idx])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.upconv4.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
