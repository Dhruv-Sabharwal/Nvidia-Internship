{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import sklearn.feature_extraction\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "%load_ext tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from fastai.layers import PixelShuffle_ICNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/workspace/data/Dhruv/pytorch/SuperResolution/Data'\n",
    "writer = SummaryWriter('runs/resunet50decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.zeros((800, 512, 512, 3))\n",
    "i = 0\n",
    "for f in os.listdir(dir + '/' + 'Train'):\n",
    "    if(f.endswith(\".png\")):\n",
    "        train_images[i] = (skimage.transform.resize(\n",
    "            skimage.io.imread(dir + '/Train/' + f),(512,512), mode ='constant'))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/workspace/data/Dhruv/pytorch/SuperResolution/ImagenetData/content/train'\n",
    "train_images1 = np.zeros((7276, 300, 300, 3))\n",
    "i = 0\n",
    "for f in os.listdir(dir):\n",
    "    if(f.endswith(\".jpg\")):\n",
    "        train_images1[i] = (skimage.transform.resize(\n",
    "            skimage.io.imread(dir + '/' + f),(300,300), mode ='constant'))\n",
    "        i += 1\n",
    "np.random.shuffle(train_images1)\n",
    "test_images1 = train_images1[5776:]\n",
    "train_images1 = train_images1[:5776]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract 64x64 patches from the images. 20 patches from each image.\n",
    "def patchExtract(images, patch_size=(128, 128), max_patches=5):\n",
    "    pe = sklearn.feature_extraction.image.PatchExtractor(patch_size=patch_size, max_patches = max_patches)\n",
    "    pe_fit = pe.fit(images)\n",
    "    pe_trans = pe.transform(images)\n",
    "    return pe_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/workspace/data/Dhruv/pytorch/SuperResolution/Data'\n",
    "test_images = np.zeros((100, 512, 512, 3))\n",
    "i = 0\n",
    "for f in os.listdir(dir + '/' + 'Validation'):\n",
    "    if(f.endswith(\".png\")):\n",
    "        test_images[i] = (skimage.transform.resize(\n",
    "            skimage.io.imread(dir + '/Validation/' + f),(512,512), mode ='constant'))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images1 = patchExtract(train_images1, max_patches=3)\n",
    "test_images1 = patchExtract(test_images1, max_patches=3)\n",
    "train_images = patchExtract(train_images)\n",
    "test_images = patchExtract(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape)\n",
    "print(test_images.shape)\n",
    "print(train_images1.shape)\n",
    "print(test_images1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.concatenate((train_images, train_images1), axis=0)\n",
    "test_images = np.concatenate((test_images, test_images1), axis=0)\n",
    "np.random.shuffle(train_images)\n",
    "np.random.shuffle(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bicubicDownsample(images, scale_factor=0.5):\n",
    "    out = torch.nn.functional.interpolate(images, scale_factor=scale_factor, mode='bicubic', align_corners=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = torch.from_numpy(train_images).permute(0,3,1,2)\n",
    "y_tr = y_tr.float()\n",
    "y_te = torch.from_numpy(test_images).permute(0,3,1,2)\n",
    "y_te = y_te.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_images\n",
    "del test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = bicubicDownsample(y_tr)\n",
    "x_tr = x_tr.float()\n",
    "x_te = bicubicDownsample(y_te)\n",
    "x_te = x_te.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = y_tr.contiguous()\n",
    "y_te = y_te.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_tr.is_contiguous())\n",
    "print(x_te.is_contiguous())\n",
    "print(y_tr.is_contiguous())\n",
    "print(y_te.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom training dataset\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.x = x_tr\n",
    "        self.y = y_tr\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            return self.transform(self.x[index]), self.y[index]\n",
    "        else:\n",
    "            return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "# Creating custom testing dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.x = x_te\n",
    "        self.y = y_te\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            return self.transform(self.x[index]), self.y[index]\n",
    "        else:\n",
    "            return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset()\n",
    "test_dataset = TestDataset()\n",
    "\n",
    "# Implementing train loader to split the data into batches\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, # data reshuffled at every epoch\n",
    "                          num_workers=2) # Use several subprocesses to load the data\n",
    "\n",
    "# Implementing train loader to split the data into batches\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, # data reshuffled at every epoch\n",
    "                          num_workers=2) # Use several subprocesses to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "n_samples = len(train_dataset)\n",
    "n_iterations = math.ceil(n_samples/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        if(in_channels != out_channels):\n",
    "            self.skip = nn.Conv2d(in_channels, out_channels, kernel_size = 1) # Skip connection\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if(self.in_channels != self.out_channels):\n",
    "            skip_x = self.skip(x)\n",
    "        else:\n",
    "            skip_x = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        added_x = skip_x + x  # Element-wise addition of skip connection filters and residual filters\n",
    "        return self.relu(added_x)\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_blocks):\n",
    "        super().__init__()\n",
    "        self.upsample = PixelShuffle_ICNR(in_channels, in_channels//2, scale=2)\n",
    "        self.blocks = self.make_layer(in_channels, out_channels, n_blocks)\n",
    "    \n",
    "    def make_layer(self, in_channels, out_channels, n_blocks):\n",
    "        Blocks = []\n",
    "        Blocks.append(DecoderBlock(in_channels, in_channels))\n",
    "        Blocks.append(DecoderBlock(in_channels, out_channels))\n",
    "        for _ in range(2, n_blocks):\n",
    "            Blocks.append(DecoderBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*Blocks)\n",
    "            \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upsample(x1)\n",
    "        x = torch.cat((x2,x1), dim=1)\n",
    "        x = self.blocks(x)\n",
    "        return x\n",
    "\n",
    "class FinalDecoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_blocks):\n",
    "        super().__init__()\n",
    "        self.blocks = self.make_layer(in_channels, out_channels, n_blocks)\n",
    "    \n",
    "    def make_layer(self, in_channels, out_channels, n_blocks):\n",
    "        Blocks = []\n",
    "        Blocks.append(DecoderBlock(in_channels, in_channels))\n",
    "        Blocks.append(DecoderBlock(in_channels, out_channels))\n",
    "        for _ in range(2, n_blocks):\n",
    "            Blocks.append(DecoderBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*Blocks)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResUNet(nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = torchvision.models.resnet50(pretrained=True, progress=False)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "        \n",
    "        # Encoder path\n",
    "        self.in_layer1 = self.base_layers[0]\n",
    "        self.in_layer2 = nn.Sequential(*self.base_layers[1:4])\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[4])\n",
    "        self.layer2 = nn.Sequential(*self.base_layers[5])\n",
    "        self.layer3 = nn.Sequential(*self.base_layers[6])\n",
    "        self.layer4 = nn.Sequential(*self.base_layers[7])\n",
    "        \n",
    "        # Cross path\n",
    "        self.down_in1 = nn.Conv2d(64, 128 ,kernel_size=1)\n",
    "        self.down_up = nn.Conv2d(3, 64, kernel_size=1)\n",
    "        \n",
    "        # Decoder path\n",
    "        self.up1 = DecoderLayer(2048, 1024, 3)\n",
    "        self.up2 = DecoderLayer(1024, 512, 6)\n",
    "        self.up3 = DecoderLayer(512, 256, 4)\n",
    "        self.up4 = DecoderLayer(256, 128, 3)\n",
    "        self.up5 = DecoderLayer(128, 64, 3)\n",
    "        \n",
    "        self.out_layer = FinalDecoderLayer(64, 3, 3)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Encoder path\n",
    "        x_up = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        x_in1 = self.in_layer1(x_up)\n",
    "        x_in2 = self.in_layer2(x_in1) # This is of same size as x_l1 so not used\n",
    "        x_l1 = self.layer1(x_in2)\n",
    "        x_l2 = self.layer2(x_l1)\n",
    "        x_l3 = self.layer3(x_l2)\n",
    "        x_l4 = self.layer4(x_l3)\n",
    "        \n",
    "        # Decoder path\n",
    "        x = self.up1(x_l4, x_l3)\n",
    "        x = self.up2(x, x_l2)\n",
    "        x = self.up3(x, x_l1)\n",
    "        x_in1 = self.down_in1(x_in1)\n",
    "        x = self.up4(x, x_in1)\n",
    "        x_up = self.down_up(x_up)\n",
    "        x = self.up5(x, x_up)\n",
    "        x = self.out_layer(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Loss Function (Perceptual Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = torchvision.models.vgg16(pretrained=True, progress=False)\n",
    "        features = model.features\n",
    "        self.relu1_2 = nn.Sequential()\n",
    "        self.relu2_2 = nn.Sequential()\n",
    "        self.relu3_3 = nn.Sequential()\n",
    "        self.relu4_3 = nn.Sequential()\n",
    "        for i in range(4):\n",
    "            self.relu1_2.add_module(name=\"relu1_2_\"+str(i+1), module=features[i])\n",
    "        for i in range(4, 9):\n",
    "            self.relu2_2.add_module(name=\"relu2_2_\"+str(i-3), module=features[i])\n",
    "        for i in range(9, 16):\n",
    "            self.relu3_3.add_module(name=\"relu3_3_\"+str(i-8), module=features[i])\n",
    "        for i in range(16, 23):\n",
    "            self.relu4_3.add_module(name=\"relu4_3_\"+str(i-15), module=features[i])      \n",
    "        # Setting requires_grad=False to fix the perceptual loss model parameters \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out_relu1_2 = self.relu1_2(x)\n",
    "        out_relu2_2 = self.relu2_2(out_relu1_2)\n",
    "        out_relu3_3 = self.relu3_3(out_relu2_2)\n",
    "        out_relu4_3 = self.relu4_3(out_relu3_3)\n",
    "        return out_relu1_2, out_relu2_2, out_relu3_3, out_relu4_3\n",
    "    \n",
    "# Function to calculate Gram matrix\n",
    "def gram(x):\n",
    "    (N, C, H, W) = x.shape\n",
    "    psy = x.view(N, C, H*W)\n",
    "    psy_T = psy.transpose(1, 2)\n",
    "    G = torch.bmm(psy, psy_T) / (C*H*W)  # Should we divide by N here? Or does batch matric multiplication do that on it's own? \n",
    "    return G\n",
    "\n",
    "def TVR(x, TV_WEIGHT=1e-8):\n",
    "    diff_i = torch.sum(torch.abs(x[:, :, :, 1:] - x[:, :, :, :-1]))\n",
    "    diff_j = torch.sum(torch.abs(x[:, :, 1:, :] - x[:, :, :-1, :]))\n",
    "    tv_loss = TV_WEIGHT*(diff_i + diff_j)\n",
    "    return tv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGGLoss = VGGPerceptualLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PerceptualLoss(x, y, STYLE_WEIGHT=1e-4, CONTENT_WEIGHT=1e-4, PIXEL_WEIGHT=1):\n",
    "    \n",
    "    x_features = VGGLoss(x)\n",
    "    y_features = VGGLoss(y)\n",
    "    \n",
    "    # Calculating per-pixel loss\n",
    "    C = y.shape[1]\n",
    "    H = y.shape[2]\n",
    "    W = y.shape[3]\n",
    "    pixel_loss =  F.l1_loss(x, y, reduction='sum') / (C*H*W)\n",
    "    #print(pixel_loss)\n",
    "    \n",
    "    # Calculating Total variation regularization value\n",
    "    tvr_loss = TVR(x)\n",
    "    #print(tvr_loss)\n",
    "    \n",
    "    # Calculating content loss\n",
    "    #weights = [0.25, 0.25, 0.25]\n",
    "    content_loss = 0.0\n",
    "    for i in range(2,4):\n",
    "        C = y_features[i].shape[1]\n",
    "        H = y_features[i].shape[2]\n",
    "        W = y_features[i].shape[3]\n",
    "        content_loss += (F.l1_loss(y_features[i], x_features[i], reduction='sum') / (C*H*W))\n",
    "        #print('c',content_loss)\n",
    "    #print(content_loss)\n",
    "    '''\n",
    "    # Calculating content loss\n",
    "    C = y_features[2].shape[1]\n",
    "    H = y_features[2].shape[2]\n",
    "    W = y_features[2].shape[3]\n",
    "    content_loss = F.l1_loss(x_features[2], y_features[2], reduction='sum') / (C*H*W)\n",
    "    #print(content_loss)\n",
    "    '''\n",
    "    # Calculating Style loss\n",
    "    style_loss = 0.0\n",
    "    for i in range(4):\n",
    "        C = y_features[i].shape[1]\n",
    "        H = y_features[i].shape[2]\n",
    "        W = y_features[i].shape[3]\n",
    "        style_loss += F.l1_loss(gram(x_features[i]), gram(y_features[i]), reduction='sum')\n",
    "        #print('s ',style_loss)\n",
    "    #print(style_loss)\n",
    "    total_loss = STYLE_WEIGHT*style_loss + CONTENT_WEIGHT*content_loss + PIXEL_WEIGHT*pixel_loss + tvr_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing checkpoints\n",
    "def save_checkpoint_best(epoch, model):\n",
    "    print(\"Saving best model\")\n",
    "    PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/BestModel/best_model_\"+str(epoch)+\".pt\"\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, loss):  # Saving model in a way so we can load and start training again\n",
    "    PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/Models/model_\"+str(epoch)+\".pt\"\n",
    "    print(\"Saving model\")\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': loss,\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss_log = []\n",
    "val_loss_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResUNet(3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, layer in model._modules.items():\n",
    "    if name in ['in_layer1', 'in_layer2', 'layer1', 'layer2', 'layer3', 'layer4']:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adadelta(model.parameters())\n",
    "#loss = PerceptualLoss used directly in training loop\n",
    "\n",
    "example = iter(train_loader)\n",
    "example_data, example_target = example.next()\n",
    "writer.add_graph(model, example_data.to(device))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=7, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_model():\n",
    "    least_val_loss = math.inf\n",
    "    flag = False\n",
    "    for epoch in range(EPOCHS):   \n",
    "        '''\n",
    "        if((epoch != 0) and ((epoch)%20 == 0)):\n",
    "            if(flag == False):\n",
    "                flag = True\n",
    "                print(\"Setting encoder to trainable\")\n",
    "                for name, layer in model._modules.items():\n",
    "                    if name in ['in_layer1', 'in_layer2', 'layer1', 'layer2', 'layer3', 'layer4']:\n",
    "                        for param in layer.parameters():\n",
    "                            param.requires_grad = True\n",
    "            elif(flag == True):\n",
    "                flag = False\n",
    "                print(\"Setting encoder to non-trainable\")\n",
    "                for name, layer in model._modules.items():\n",
    "                    if name in ['in_layer1', 'in_layer2', 'layer1', 'layer2', 'layer3', 'layer4']:\n",
    "                        for param in layer.parameters():\n",
    "                            param.requires_grad = False\n",
    "        '''\n",
    "        beg_time = time.time() #To calculate time taken for each epoch\n",
    "\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            # Will run for 1000 iterations per epoch\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            out = model(x)\n",
    "            #Calculating loss\n",
    "            loss = PerceptualLoss(out, y)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update gradients\n",
    "            optimizer.step()\n",
    "            # Get training loss\n",
    "            train_loss += loss.item()\n",
    "        tr_loss_log.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(test_loader):\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                out = model(x)\n",
    "                #Calculating loss\n",
    "                loss = PerceptualLoss(out, y)\n",
    "                # Get validation loss\n",
    "                val_loss += loss.item()\n",
    "            val_loss_log.append(val_loss)\n",
    "        model.train()\n",
    "\n",
    "        #scheduler.step(val_loss)\n",
    "\n",
    "        # Saving checkpoints\n",
    "        #save_checkpoint(epoch+1, model, optimizer, val_loss)\n",
    "        if(val_loss < least_val_loss):\n",
    "            save_checkpoint_best(epoch+1, model)\n",
    "            least_val_loss = val_loss\n",
    "\n",
    "        end_time = time.time()\n",
    "        print('Epoch: {:.0f}/{:.0f}, Time: {:.0f}m {:.0f}s, Train_Loss: {:.4f}, Val_loss: {:.4f}'.format(\n",
    "              epoch+1, EPOCHS, (end_time-beg_time)//60, (end_time-beg_time)%60, train_loss, val_loss))\n",
    "        writer.add_scalar('Training_loss', train_loss, epoch+1)\n",
    "        writer.add_scalar('Validation_loss', val_loss, epoch+1)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=runs/resunet50decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the final model\n",
    "PATH = \"/workspace/data/Dhruv/pytorch/SuperResolution/FinalModel/final_trained_model.pt\"\n",
    "print(\"Saving final model\")\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the best model (fill in the best model epoch number)\n",
    "loaded_best_model = ResUNet(3).to(device)\n",
    "checkpoint = torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/BestModel/best_model_23.pt\")\n",
    "loaded_best_model.load_state_dict(checkpoint)\n",
    "loaded_best_model.eval()\n",
    "model = loaded_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading a model with desired epoch number\n",
    "loaded_model = ResUNet(3).to(device)\n",
    "checkpoint = torch.load(\"/workspace/data/Dhruv/pytorch/SuperResolution/Models/model_8.pt\")\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model.eval()\n",
    "model = loaded_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "example = iter(test_loader)\n",
    "example_data, example_target = example.next()\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "# Downsampled\n",
    "plt.subplot(2,2,1)\n",
    "plt.title('Downsampled')\n",
    "plt.imshow(example_data[15].permute(1,2,0))\n",
    "# Bi-linear upsampled\n",
    "out_1 = F.interpolate(example_data, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('Bi-linear upsampled')\n",
    "plt.imshow(out_1[15].permute(1,2,0))\n",
    "# Model prediction\n",
    "out_2 = model(example_data.to(device))\n",
    "plt.subplot(2,2,3)\n",
    "plt.title('Prediction')\n",
    "plt.imshow(out_2[15].cpu().detach().permute(1,2,0))\n",
    "# Ground truth\n",
    "plt.subplot(2,2,4)\n",
    "plt.title('Ground truth')\n",
    "plt.imshow(example_target[15].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature maps\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, _ = test_dataset[16]\n",
    "plt.imshow(data.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find activations for all the layers of the model\n",
    "for name, layer in model._modules.items():\n",
    "  layer.register_forward_hook(get_activation(name))\n",
    "data, _ = test_dataset[16]\n",
    "data.unsqueeze_(0)\n",
    "output = model(data.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = activation['up5'].squeeze().cpu()\n",
    "for idx in range(act.size(0)):\n",
    "    plt.imshow(act[idx])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
