{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://github.com/digantamisra98/Echo for implementation/equation of activation functions.\n",
    "<br>Reference: https://echo-ai.readthedocs.io/en/latest/ for explanation of functions/parameters and graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the weighted tanh function element-wise:\n",
    "# weightedtanh(x) = tanh(x * weight)\n",
    "\n",
    "def weighted_tanh(input, weight=1, inplace=False):\n",
    "    if inplace is False:\n",
    "        return torch.tanh(weight * input)\n",
    "    else:\n",
    "        input *= weight\n",
    "        torch.tanh_(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the Swish function element-wise\n",
    "# Swish(x, beta) = x*sigmoid(beta*x) = frac{x}{(1+e^{-beta*x})}\n",
    "\n",
    "def swish(input, beta=1.25):\n",
    "    return input * torch.sigmoid(beta * input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESwish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the E-Swish function element-wise\n",
    "# ESwish(x, beta) = beta*x*sigmoid(x)\n",
    "\n",
    "def eswish(input, beta=1.375):\n",
    "    return beta * input * torch.sigmoid(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aria-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the Aria-2 function element-wise\n",
    "# Aria2(x, alpha, beta) = (1+e^{-beta*x})^{-alpha}\n",
    "\n",
    "def aria2(input, beta=0.5, alpha=1):\n",
    "    return torch.pow((1 + torch.exp(-beta * input)), -alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELiSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the ELiSH (Exponential Linear Sigmoid SquasHing) function element-wise\n",
    "# ELiSH(x) =x / (1+e^{-x}), x >= 0  \n",
    "#          =(e^{x} - 1) / (1 + e^{-x}), x < 0\n",
    "\n",
    "def elish(input):\n",
    "    return (input >= 0).float() * input * torch.sigmoid(input) + (input < 0).float() * (\n",
    "        torch.exp(input) - 1\n",
    "    ) / (torch.exp(-input) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HardELiSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the HardELiSH (Exponential Linear Sigmoid SquasHing) function element-wise\n",
    "# HardELiSH(x) = \\\\left\\\\{\\\\begin{matrix} x \\\\times max(0, min(1, (x + 1) / 2)), x \\\\geq 0 \\\\\\\\ (e^{x} - 1)\\\\times max(0, min(1, (x + 1) / 2)), x < 0 \\\\end{matrix}\\\\right.\n",
    "\n",
    "def hard_elish(input):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return (input >= 0).float() * input * torch.max(\n",
    "        torch.tensor(0.0, device=device),\n",
    "        torch.min(torch.tensor(1.0, device=device), (input + 1.0) / 2.0),\n",
    "    ) + (input < 0).float() * (\n",
    "        (torch.exp(input) - 1)\n",
    "        * torch.max(\n",
    "            torch.tensor(0.0, device=device),\n",
    "            torch.min(torch.tensor(1.0, device=device), (input + 1.0) / 2.0),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the mila function element-wise\n",
    "# mila(x) = x * tanh(softplus(beta + x)) = x * tanh(ln(1 + e^{beta + x}))\n",
    "\n",
    "def mila(input, beta=-0.25):\n",
    "    return input * torch.tanh(F.softplus(input + beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SineReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the SineReLU activation function element-wise\n",
    "# SineReLU(x, epsilon) = \\\\left\\\\{\\\\begin{matrix} x , x > 0 \\\\\\\\ epsilon * (sin(x) - cos(x)), x \\\\leq  0 \\\\end{matrix}\\\\right.\n",
    "\n",
    "def sineReLU(input, eps=0.01):\n",
    "    return (input > 0).float() * input + (input <= 0).float() * eps * (\n",
    "        torch.sin(input) - torch.cos(input)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten T-Swish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the FTS (Flatten T-Swish) activation function element-wise\n",
    "# FTS(x) = \\\\left\\\\{\\\\begin{matrix} \\\\frac{x}{1 + e^{-x}} , x \\\\geq  0 \\\\\\\\ 0, x < 0 \\\\end{matrix}\\\\right.\n",
    "\n",
    "def fts(input):\n",
    "    return torch.clamp(input / (1 + torch.exp(-input)), min=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the SQNL activation function element-wise\n",
    "# SQNL(x) = \\\\left\\\\{\\\\begin{matrix} 1, x > 2 \\\\\\\\ x - \\\\frac{x^2}{4}, 0 \\\\leq x \\\\leq 2 \\\\\\\\  x + \\\\frac{x^2}{4}, -2 \\\\leq x < 0 \\\\\\\\ -1, x < -2 \\\\end{matrix}\\\\right.\n",
    "\n",
    "def sqnl(input):\n",
    "    return (\n",
    "        (input > 2).float()\n",
    "        + (input - torch.pow(input, 2) / 4)\n",
    "        * (input >= 0).float()\n",
    "        * (input <= 2).float()\n",
    "        + (input + torch.pow(input, 2) / 4)\n",
    "        * (input < 0).float()\n",
    "        * (input >= -2).float()\n",
    "        - (input < -2).float()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the ISRU function element-wise\n",
    "# ISRU(x, alpha) = \\\\frac{x}{\\\\sqrt{1 + alpha * x^2}}\n",
    "\n",
    "def isru(input, alpha=1.0):\n",
    "    return input / (torch.sqrt(1 + alpha * torch.pow(input, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISRLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the ISRLU function element-wise\n",
    "# ISRLU(x, alpha)=\\\\left\\\\{\\\\begin{matrix} x, x\\\\geq 0 \\\\\\\\  x * (\\\\frac{1}{\\\\sqrt{1 + \\\\alpha*x^2}}), x <0 \\\\end{matrix}\\\\right.\n",
    "\n",
    "def isrlu(input, alpha=1.0):\n",
    "    return (input < 0).float() * isru(input, alpha) + (input >= 0).float() * input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bent's identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the Bent's Identity function element-wise\n",
    "# bentId(x) = x + \\\\frac{\\\\sqrt{x^{2}+1}-1}{2}\n",
    "\n",
    "def bent_id(input):\n",
    "    return input + ((torch.sqrt(torch.pow(input, 2) + 1) - 1) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the Soft Clipping function element-wise\n",
    "# SC(x) = 1 / \\\\alpha * log(\\\\frac{1 + e^{\\\\alpha * x}}{1 + e^{\\\\alpha * (x-1)}})\n",
    "\n",
    "def soft_clipping(input, alpha=0.5):\n",
    "    return (1 / alpha) * torch.log(\n",
    "        (1 + torch.exp(alpha * input)) / (1 + torch.exp(alpha * (input - 1)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure\n",
    "# BReLU is applied differently at even and odd indices\n",
    "# BReLU(x_i) = \\\\left\\\\{\\\\begin{matrix} f(x_i), i \\\\mod 2 = 0\\\\\\\\  - f(-x_i), i \\\\mod 2 \\\\neq  0 \\\\end{matrix}\\\\right.\n",
    "\n",
    "def brelu(input):\n",
    "    # get lists of odd and even indices\n",
    "    input_shape = input.shape[0]\n",
    "    even_indices = [i for i in range(0, input_shape, 2)]\n",
    "    odd_indices = [i for i in range(1, input_shape, 2)]\n",
    "\n",
    "    # clone the input tensor\n",
    "    output = input.clone()\n",
    "\n",
    "    # apply ReLU to elements where i mod 2 == 0\n",
    "    output[even_indices] = output[even_indices].clamp(min=0)\n",
    "\n",
    "    # apply inversed ReLU to inversed elements where i mod 2 != 0\n",
    "    output[odd_indices] = (\n",
    "        0 - output[odd_indices]\n",
    "    )  # reverse elements with odd indices\n",
    "    output[odd_indices] = -output[odd_indices].clamp(min=0)  # apply reversed ReLU\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure\n",
    "# APL is applied differently at each index\n",
    "# APL(x_i) = max(0,x) + \\\\sum_{s=1}^{S}{a_i^s * max(0, -x + b_i^s)}\n",
    "def apl(input, S, a, b):\n",
    "    \n",
    "    output = input.clamp(min=0)\n",
    "    \n",
    "    for s in range(S):\n",
    "        t = -input + torch.pow(b[s],s)\n",
    "        output += torch.pow(a[s],s) * t.clamp(min=0)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure\n",
    "# Applies the soft exponential function element-wise\n",
    "# SoftExponential(x, \\\\alpha) = \\\\left\\\\{\\\\begin{matrix} - \\\\frac{log(1 - \\\\alpha(x + \\\\alpha))}{\\\\alpha}, \\\\alpha < 0\\\\\\\\  x, \\\\alpha = 0\\\\\\\\  \\\\frac{e^{\\\\alpha * x} - 1}{\\\\alpha} + \\\\alpha, \\\\alpha > 0 \\\\end{matrix}\\\\right.\n",
    "\n",
    "def softExp(input, alpha = 0.0):\n",
    "    if alpha == 0.0:\n",
    "        return input\n",
    "\n",
    "    if alpha < 0.0:\n",
    "        return -torch.log(1 - alpha * (input + alpha)) / alpha\n",
    "\n",
    "    if alpha > 0.0:\n",
    "        return (torch.exp(alpha * input) - 1) / alpha + alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maxout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the Mish function element-wise\n",
    "# beta_mish(x) = x * tanh(ln(1 + e^{x}))\n",
    "\n",
    "def mish(input):\n",
    "    return input * torch.tanh(torch.log(1 + torch.exp(input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta Mish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the Beta Mish function element-wise\n",
    "# beta_mish(x, beta) = x * tanh(ln((1 + e^{x})^{beta}))\n",
    "\n",
    "def beta_mish(input, beta=1.5):\n",
    "    return input * torch.tanh(torch.log(torch.pow((1 + torch.exp(input)), beta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeCun's Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the Le Cun's Tanh function element-wise\n",
    "# lecun_tanh(x) = 1.7159 * tanh((2/3) * input)\n",
    "\n",
    "def lecun_tanh(input):\n",
    "    return 1.7159 * torch.tanh((2 * input) / 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SiLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the Sigmoid Linear Unit (SiLU) function element-wise\n",
    "# SiLU(x) = x * sigmoid(x)\n",
    "\n",
    "def silu(input, inplace=False):\n",
    "    if inplace:\n",
    "        result = input.clone()\n",
    "        torch.sigmoid_(input)\n",
    "        input *= result\n",
    "    else:\n",
    "        return input * torch.sigmoid(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the natural logarithm ReLU activation function element-wise\n",
    "\n",
    "def nl_relu(input, beta=1.):\n",
    "    return (input > 0).float() * torch.log(1. + beta * torch.clamp(input, min = 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
