{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNET with ResNet building blocks. Resnet blocks help in identity mappings, allowing us to create deeper models and preventing the model from running into the problems of vanishing/exploding gradients. The training error in ResNet models can be decreased further than plain models i.e. the saturation point is shifted towards lower values of training error. ResNet blocks also help us in getting a smoother loss curve. ResNet combined with U-Net architecture leads to the amalgamation of the advantageous properties of both the models in the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size = 1) # Skip connection\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip_x = self.skip(x)\n",
    "        conv_x = self.double_conv(x)\n",
    "        added_x = skip_x + conv_x  # Element-wise addition of skip connection filters and residual filters\n",
    "        return F.relu_(added_x) # Inplace functional version of relu\n",
    "\n",
    "class UpConcatConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1)  # 1x1 convolution to reduce num of channels to half\n",
    "        )\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upsample(x1)\n",
    "        # No need to crop the feature maps from the corresponding contracting layer since we using padding in DoubleConv\n",
    "        x = torch.cat((x2, x1), dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.dconv1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.dconv2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.dconv3 = DoubleConv(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.dconv4 = DoubleConv(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        self.dconv5 = DoubleConv(512, 1024)\n",
    "        self.up1 = UpConcatConv(1024, 512)\n",
    "        self.up2 = UpConcatConv(512, 256)\n",
    "        self.up3 = UpConcatConv(256, 128)\n",
    "        self.up4 = UpConcatConv(128, 64)\n",
    "        self.outconv = nn.Conv2d(64, out_channels, kernel_size=1)  # Final 1x1 convolution\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.dconv1(x)\n",
    "        x2 = self.pool1(x1)\n",
    "        x2 = self.dconv2(x2)\n",
    "        x3 = self.pool2(x2)\n",
    "        x3 = self.dconv3(x3)\n",
    "        x4 = self.pool3(x3)\n",
    "        x4 = self.dconv4(x4)\n",
    "        x5 = self.pool4(x4)\n",
    "        x5 = self.dconv5(x5)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        return self.outconv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(10, 3, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 128, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
